{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNc60V9bAHhPfrHj/LahyMf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/project_AIEngineer/blob/main/All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x9SQXVqzkCa",
        "outputId": "8c7361cc-93da-4232-a5b2-3fb3df50d500"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "6OFvD2T1jLkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "2Bn2IDK0B-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client"
      ],
      "metadata": {
        "id": "I8bIgg5P2Jw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#detect crop and segment"
      ],
      "metadata": {
        "id": "Jnc83w_hyjZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your YOLOv8 segmentation model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_segment.pt')\n",
        "\n",
        "# Define the input directory\n",
        "input_dir = '/content/data 50 class add_augment'\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "collection_name = \"vector_drug\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-base-patch16\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Function to generate image embedding\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "# Function to display image\n",
        "def display_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Query Image: {os.path.basename(image_path)}')\n",
        "    plt.show()\n",
        "\n",
        "# Function to search for similar items in Qdrant\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        print(\"Not enough results found.\")\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "# Function to process detection, cropping, segmentation, and search\n",
        "def detect_crop_and_segment(image_path):\n",
        "    results = model(image_path)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "        masks = result.masks.data.cpu().detach().numpy() if result.masks is not None else None\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, label in enumerate(labels):\n",
        "            if scores[i] >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im = Image.open(image_path).convert('RGB')\n",
        "                im_np = np.array(im)\n",
        "                im_crop = im_np[y1:y2, x1:x2]\n",
        "\n",
        "                if masks is not None:\n",
        "                    mask = masks[i]\n",
        "                    mask_resized = cv2.resize(mask, (x2-x1, y2-y1), interpolation=cv2.INTER_NEAREST)\n",
        "                    im_segmented = cv2.bitwise_and(im_crop, im_crop, mask=mask_resized.astype(np.uint8))\n",
        "                else:\n",
        "                    im_segmented = im_crop\n",
        "\n",
        "                return Image.fromarray(im_segmented)\n",
        "\n",
        "    print(f\"No target objects detected in '{image_path}'.\")\n",
        "    return None\n",
        "\n",
        "# Function to apply conditions on search results and display output\n",
        "def apply_conditions_and_display(image_path):\n",
        "    segmented_image = detect_crop_and_segment(image_path)\n",
        "    if segmented_image is None:\n",
        "        return\n",
        "\n",
        "    top_1, top_2 = search_similar_items(segmented_image, qdrant_client, collection_name)\n",
        "    if top_1 is None or top_2 is None:\n",
        "        return\n",
        "\n",
        "    score_1 = top_1.score\n",
        "    score_2 = top_2.score\n",
        "\n",
        "    # Condition 1: Score > 0.9 and difference between top 1 and top 2 is > 2%\n",
        "    if score_1 > 0.9:\n",
        "        if (score_1 - score_2) >= 0.02:\n",
        "            print(f\"Class Name: {top_1.payload.get('class', 'Unknown')}, Score: {score_1}\")\n",
        "        else:\n",
        "            print(\"Top 1 and top 2 scores are close. Please retake the image.\")\n",
        "\n",
        "    # Condition 2: 0.85 < score <= 0.9\n",
        "    elif 0.85 < score_1 <= 0.9:\n",
        "        print(\"This drug might be in the class. Please retake the image.\")\n",
        "\n",
        "    # Condition 3: score <= 0.85\n",
        "    else:\n",
        "        print(\"This drug is not in the class. Saving as a new class.\")\n",
        "        # Add the logic for saving as a new class here\n",
        "\n",
        "# Process all images in the input directory\n",
        "if os.path.isdir(input_dir):\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for file_name in sorted(files):\n",
        "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                apply_conditions_and_display(file_path)\n",
        "else:\n",
        "    print(f\"Invalid path: {input_dir}\")"
      ],
      "metadata": {
        "id": "WdH3xJYykfib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#detect and crop"
      ],
      "metadata": {
        "id": "Fwsj7AwWyrdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_segment.pt')\n",
        "\n",
        "# Define the input directory\n",
        "input_dir = '/content/data 50 class add_augment'\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "collection_name = \"vector_drug\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-base-patch16\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Function to generate image embedding\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "# Function to display image\n",
        "def display_image(image_path):\n",
        "    img = Image.open(image_path)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(f'Query Image: {os.path.basename(image_path)}')\n",
        "    plt.show()\n",
        "\n",
        "# Function to search for similar items in Qdrant\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        print(\"Not enough results found.\")\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "# Function to process detection and cropping\n",
        "def detect_and_crop(image_path):\n",
        "    results = model(image_path)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, label in enumerate(labels):\n",
        "            if scores[i] >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im = Image.open(image_path).convert('RGB')\n",
        "                im_crop = im.crop((x1, y1, x2, y2))\n",
        "\n",
        "                return im_crop\n",
        "\n",
        "    print(f\"No target objects detected in '{image_path}'.\")\n",
        "    return None\n",
        "\n",
        "# Function to apply conditions on search results and display output\n",
        "def apply_conditions_and_display(image_path):\n",
        "    cropped_image = detect_and_crop(image_path)\n",
        "    if cropped_image is None:\n",
        "        return\n",
        "\n",
        "    top_1, top_2 = search_similar_items(cropped_image, qdrant_client, collection_name)\n",
        "    if top_1 is None or top_2 is None:\n",
        "        return\n",
        "\n",
        "    score_1 = top_1.score\n",
        "    score_2 = top_2.score\n",
        "\n",
        "    # Condition 1: Score > 0.9 and difference between top 1 and top 2 is > 2%\n",
        "    if score_1 > 0.9:\n",
        "        if (score_1 - score_2) >= 0.02:\n",
        "            print(f\"Class Name: {top_1.payload.get('class', 'Unknown')}, Score: {score_1}\")\n",
        "        else:\n",
        "            print(\"Top 1 and top 2 scores are close. Please retake the image.\")\n",
        "\n",
        "    # Condition 2: 0.85 < score <= 0.9\n",
        "    elif 0.85 < score_1 <= 0.9:\n",
        "        print(\"This drug might be in the class. Please retake the image.\")\n",
        "\n",
        "    # Condition 3: score <= 0.85\n",
        "    else:\n",
        "        print(\"This drug is not in the class. Saving as a new class.\")\n",
        "        # Add the logic for saving as a new class here\n",
        "\n",
        "# Process all images in the input directory\n",
        "if os.path.isdir(input_dir):\n",
        "    for root, _, files in os.walk(input_dir):\n",
        "        for file_name in sorted(files):\n",
        "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                file_path = os.path.join(root, file_name)\n",
        "                apply_conditions_and_display(file_path)\n",
        "else:\n",
        "    print(f\"Invalid path: {input_dir}\")"
      ],
      "metadata": {
        "id": "KzKew5fJx4o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#รวม"
      ],
      "metadata": {
        "id": "6x_w_uAKzcU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. เอาภาพเข้ามา\n",
        "2. ครอปภาพ\n",
        "3. เอาภาพไปหาความเหมือนในqdrant\n",
        "4. เอาผลค่าความเหมือนมาเข้าเงื่อนไข\n",
        "5. แสดงผลลัพท์ภาพต้นฉบับ+ชื่อคลาสที่ทำนายได้"
      ],
      "metadata": {
        "id": "nem5bWCYy088"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_new.pt')\n",
        "\n",
        "# Define the input image path\n",
        "input_image_path = '/content/Lanzaar100mg-n.jpg'\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "collection_name = \"vector_CLIP\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Function to generate image embedding\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "# Function to search for similar items in Qdrant\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        print(\"Not enough results found.\")\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "# Function to process detection and cropping\n",
        "def detect_and_crop(image_path):\n",
        "    results = model(image_path)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, label in enumerate(labels):\n",
        "            if scores[i] >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im = Image.open(image_path).convert('RGB')\n",
        "                im_crop = im.crop((x1, y1, x2, y2))\n",
        "\n",
        "                return im_crop\n",
        "\n",
        "    print(f\"No target objects detected in '{image_path}'.\")\n",
        "    return None\n",
        "\n",
        "# Function to apply conditions, display output, and show cropped image with similarity score\n",
        "def apply_conditions_and_display(image_path):\n",
        "    cropped_image = detect_and_crop(image_path)\n",
        "    if cropped_image is None:\n",
        "        return\n",
        "\n",
        "    top_1, top_2 = search_similar_items(cropped_image, qdrant_client, collection_name)\n",
        "    if top_1 is None or top_2 is None:\n",
        "        return\n",
        "\n",
        "    score_1 = top_1.score\n",
        "    score_2 = top_2.score\n",
        "    class_name = top_1.payload.get('class', 'Unknown')\n",
        "\n",
        "    # Condition 1: Score > 0.9 and difference between top 1 and top 2 is > 2%\n",
        "    if score_1 > 0.9:\n",
        "        if (score_1 - score_2) >= 0.02:\n",
        "            prediction = f\"Class Name: {class_name}, Score 1: {score_1:.4f}, Score 2: {score_2:.4f}\"\n",
        "        else:\n",
        "            prediction = f\"Top 1 and top 2 scores are close (Score 1: {score_1:.4f}, Score 2: {score_2:.4f}). Please retake the image.\"\n",
        "\n",
        "    # Condition 2: 0.85 < score < 0.9\n",
        "    elif 0.85 < score_1 < 0.9:\n",
        "        prediction = f\"This drug might be in the class. Score 1: {score_1:.4f}. Please retake the image.\"\n",
        "\n",
        "    # Condition 3: score < 0.85\n",
        "    else:\n",
        "        prediction = f\"This drug is not in the class. Score 1: {score_1:.4f}. Saving as a new class.\"\n",
        "        # Add the logic for saving as a new class here\n",
        "\n",
        "    # Display the cropped image with similarity score\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cropped_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(prediction)\n",
        "    plt.show()\n",
        "\n",
        "# Process the single image\n",
        "apply_conditions_and_display(input_image_path)"
      ],
      "metadata": {
        "id": "Af3z1-Vpy6sY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
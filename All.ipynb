{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/project_AIEngineer/blob/main/All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x9SQXVqzkCa",
        "outputId": "c487ba59-953c-4789-8bc6-6e32554550f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "6OFvD2T1jLkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "2Bn2IDK0B-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client"
      ],
      "metadata": {
        "id": "I8bIgg5P2Jw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#รวม"
      ],
      "metadata": {
        "id": "6x_w_uAKzcU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. เอาภาพเข้ามา\n",
        "2. ครอปภาพ\n",
        "3. เอาภาพไปหาความเหมือนในqdrant\n",
        "4. เอาผลค่าความเหมือนมาเข้าเงื่อนไข\n",
        "5. แสดงผลลัพท์ภาพต้นฉบับ+ชื่อคลาสที่ทำนายได้"
      ],
      "metadata": {
        "id": "nem5bWCYy088"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_new.pt')\n",
        "\n",
        "# Define the input image path\n",
        "input_image_path = '/content/Oseni25_15mg-n.jpg'\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "collection_name = \"vector_CLIP\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Function to generate image embedding\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "# Function to search for similar items in Qdrant\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        print(\"Not enough results found.\")\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "# Function to process detection and cropping\n",
        "def detect_and_crop(image_path):\n",
        "    results = model(image_path)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, label in enumerate(labels):\n",
        "            if scores[i] >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im = Image.open(image_path).convert('RGB')\n",
        "                im_crop = im.crop((x1, y1, x2, y2))\n",
        "\n",
        "                return im_crop\n",
        "\n",
        "    print(f\"No target objects detected in '{image_path}'.\")\n",
        "    return None\n",
        "\n",
        "# Function to apply conditions, display output, and show cropped image with similarity score\n",
        "def apply_conditions_and_display(image_path):\n",
        "    cropped_image = detect_and_crop(image_path)\n",
        "    if cropped_image is None:\n",
        "        return\n",
        "\n",
        "    top_1, top_2 = search_similar_items(cropped_image, qdrant_client, collection_name)\n",
        "    if top_1 is None or top_2 is None:\n",
        "        return\n",
        "\n",
        "    score_1 = top_1.score\n",
        "    score_2 = top_2.score\n",
        "    class_name = top_1.payload.get('class', 'Unknown')\n",
        "\n",
        "    # Condition 1: Score > 0.9 and difference between top 1 and top 2 is > 2%\n",
        "    if score_1 > 0.9:\n",
        "        if (score_1 - score_2) >= 0.02:\n",
        "            prediction = f\"Class Name: {class_name}, Score 1: {score_1:.4f}, Score 2: {score_2:.4f}\"\n",
        "        else:\n",
        "            prediction = f\"Top 1 and top 2 scores are close (Score 1: {score_1:.4f}, Score 2: {score_2:.4f}). Please retake the image.\"\n",
        "\n",
        "    # Condition 2: 0.85 < score < 0.9\n",
        "    elif 0.85 < score_1 < 0.9:\n",
        "        prediction = f\"This drug might be in the class. Score 1: {score_1:.4f}. Please retake the image.\"\n",
        "\n",
        "    # Condition 3: score < 0.85\n",
        "    else:\n",
        "        prediction = f\"This drug is not in the class. Score 1: {score_1:.4f}. Saving as a new class.\"\n",
        "        # Add the logic for saving as a new class here\n",
        "\n",
        "    # Display the cropped image with similarity score\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cropped_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(prediction)\n",
        "    plt.show()\n",
        "\n",
        "# Process the single image\n",
        "apply_conditions_and_display(input_image_path)"
      ],
      "metadata": {
        "id": "Af3z1-Vpy6sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pXYkJmvoSH8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# แทน\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_new.pt')\n",
        "\n",
        "# Define the input image path\n",
        "input_image_path = '/content/Oseni25_15mg-n.jpg'\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url = \"https://a63ffbf5-5568-46dd-9ec3-98751a51a998.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key = \"S0QgrdtYHTC8f_53Nes2uJ4gWoxbPnIwkujhfRlwcWA_MOvuGseLXw\"\n",
        ")\n",
        "collection_name = \"medicine50classClipModel\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Function to generate image embedding\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "# Function to search for similar items in Qdrant\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        print(\"Not enough results found.\")\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "# Function to process detection and cropping\n",
        "def detect_and_crop(image_path):\n",
        "    results = model(image_path)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, label in enumerate(labels):\n",
        "            if scores[i] >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im = Image.open(image_path).convert('RGB')\n",
        "                im_crop = im.crop((x1, y1, x2, y2))\n",
        "\n",
        "                return im_crop\n",
        "\n",
        "    print(f\"No target objects detected in '{image_path}'.\")\n",
        "    return None\n",
        "\n",
        "# Function to apply conditions, display output, and show cropped image with similarity score\n",
        "def apply_conditions_and_display(image_path):\n",
        "    cropped_image = detect_and_crop(image_path)\n",
        "    if cropped_image is None:\n",
        "        return\n",
        "\n",
        "    top_1, top_2 = search_similar_items(cropped_image, qdrant_client, collection_name)\n",
        "    if top_1 is None or top_2 is None:\n",
        "        return\n",
        "\n",
        "    score_1 = top_1.score\n",
        "    score_2 = top_2.score\n",
        "    class_name = top_1.payload.get('class', 'Unknown')\n",
        "\n",
        "    # Condition 1: Score > 0.9 and difference between top 1 and top 2 is > 2%\n",
        "    if score_1 > 0.9:\n",
        "        if (score_1 - score_2) >= 0.02:\n",
        "            prediction = f\"Class Name: {class_name}, Score 1: {score_1:.4f}, Score 2: {score_2:.4f}\"\n",
        "        else:\n",
        "            prediction = f\"Top 1 and top 2 scores are close (Score 1: {score_1:.4f}, Score 2: {score_2:.4f}). Please retake the image.\"\n",
        "\n",
        "    # Condition 2: 0.85 < score < 0.9\n",
        "    elif 0.85 < score_1 < 0.9:\n",
        "        prediction = f\"This drug might be in the class. Score 1: {score_1:.4f}. Please retake the image.\"\n",
        "\n",
        "    # Condition 3: score < 0.85\n",
        "    else:\n",
        "        prediction = f\"This drug is not in the class. Score 1: {score_1:.4f}. Saving as a new class.\"\n",
        "        # Add the logic for saving as a new class here\n",
        "\n",
        "    # Display the cropped image with similarity score\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(cropped_image)\n",
        "    plt.axis('off')\n",
        "    plt.title(prediction)\n",
        "    plt.show()\n",
        "\n",
        "# Process the single image\n",
        "apply_conditions_and_display(input_image_path)"
      ],
      "metadata": {
        "id": "jz6czzOiSH0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import streamlit as st\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('model.pt')\n",
        "\n",
        "# Connect to Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url = \"https://a63ffbf5-5568-46dd-9ec3-98751a51a998.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key = \"S0QgrdtYHTC8f_53Nes2uJ4gWoxbPnIwkujhfRlwcWA_MOvuGseLXw\"\n",
        ")\n",
        "collection_name = \"medicine50classClipModel\"\n",
        "\n",
        "# Load CLIPModel and processor for embedding generation\n",
        "clip_model_name = \"openai/clip-vit-large-patch14\"\n",
        "clip_model = CLIPModel.from_pretrained(clip_model_name)\n",
        "clip_processor = CLIPProcessor.from_pretrained(clip_model_name)\n",
        "\n",
        "# Function to generate image embedding\n",
        "def image_embedding(image):\n",
        "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model.get_image_features(**inputs)\n",
        "    return outputs[0].cpu().numpy()\n",
        "\n",
        "# Function to search for similar items in Qdrant\n",
        "def search_similar_items(query_image, client, collection_name):\n",
        "    query_embedding = image_embedding(query_image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding.tolist(),\n",
        "        limit=2  # Get top 2 results\n",
        "    )\n",
        "\n",
        "    if len(search_result) < 2:\n",
        "        st.write(\"Not enough results found.\")\n",
        "        return None, None\n",
        "\n",
        "    return search_result[0], search_result[1]\n",
        "\n",
        "# Function to process detection and cropping\n",
        "def detect_and_crop(image_file):\n",
        "    # Load image using PIL from file-like object\n",
        "    image = Image.open(image_file).convert('RGB')\n",
        "\n",
        "    results = model(image)\n",
        "    for result in results:\n",
        "        boxes = result.boxes.xyxy.cpu().detach().numpy()\n",
        "        labels = result.boxes.cls.cpu().detach().numpy()\n",
        "        scores = result.boxes.conf.cpu().detach().numpy()\n",
        "\n",
        "        threshold = 0.3\n",
        "        for i, score in enumerate(scores):\n",
        "            if score >= threshold:\n",
        "                x1, y1, x2, y2 = map(int, boxes[i])\n",
        "                im_crop = image.crop((x1, y1, x2, y2))\n",
        "                return im_crop\n",
        "\n",
        "    st.write(\"No target objects detected.\")\n",
        "    return None\n",
        "\n",
        "# Streamlit UI\n",
        "st.title('Drug Identification and Classification')\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=['jpg', 'png', 'jpeg'])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    st.image(uploaded_file, caption='Uploaded Image', use_column_width=True)\n",
        "\n",
        "    # Process image\n",
        "    cropped_image = detect_and_crop(uploaded_file)\n",
        "\n",
        "    if cropped_image:\n",
        "        top_1, top_2 = search_similar_items(cropped_image, qdrant_client, collection_name)\n",
        "        if top_1 and top_2:\n",
        "            score_1 = top_1.score\n",
        "            score_2 = top_2.score\n",
        "            class_name = top_1.payload.get('class', 'Unknown')\n",
        "\n",
        "            # Apply conditions\n",
        "            if score_1 > 0.9:\n",
        "                if (score_1 - score_2) >= 0.02:\n",
        "                    prediction = f\"Class Name: {class_name}, Score 1: {score_1:.4f}, Score 2: {score_2:.4f}\"\n",
        "                else:\n",
        "                    prediction = f\"Top 1 and top 2 scores are close (Score 1: {score_1:.4f}, Score 2: {score_2:.4f}). Please retake the image.\"\n",
        "\n",
        "            elif 0.85 < score_1 < 0.9:\n",
        "                prediction = f\"This drug might be in the class. Score 1: {score_1:.4f}. Please retake the image.\"\n",
        "\n",
        "            else:\n",
        "                prediction = f\"This drug is not in the class. Score 1: {score_1:.4f}. Saving as a new class.\"\n",
        "\n",
        "            st.write(prediction)\n",
        "            st.image(cropped_image, caption='Cropped Image', use_column_width=True)"
      ],
      "metadata": {
        "id": "C-5K3dZmzouv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ใช้อันนี้\n",
        "import io\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from qdrant_client import QdrantClient\n",
        "import streamlit as st\n",
        "import os\n",
        "\n",
        "# Initialize the models and Qdrant client (using your existing setup)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "yolo_model_path = r\"C:\\Users\\Admin\\Documents\\INET\\Drug\\Medicine_StreamlitApp\\best_segment.pt\"\n",
        "model_detection = YOLO(yolo_model_path)\n",
        "model_segmentation = YOLO(yolo_model_path)\n",
        "qdrant_url = \"https://a63ffbf5-5568-46dd-9ec3-98751a51a998.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
        "api_key = \"S0QgrdtYHTC8f_53Nes2uJ4gWoxbPnIwkujhfRlwcWA_MOvuGseLXw\"\n",
        "collection_name = \"medicine50classClipModel1.1\"\n",
        "client = QdrantClient(url=qdrant_url, api_key=api_key)\n",
        "\n",
        "def detect_and_crop_objects(image):\n",
        "    results = model_detection(image)\n",
        "    cropped_images = []\n",
        "    for result in results:\n",
        "        boxes = result.boxes\n",
        "        for box in boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0]\n",
        "            cropped_image = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "            cropped_images.append(cropped_image)\n",
        "    return cropped_images\n",
        "\n",
        "def segment_background(image):\n",
        "    results = model_segmentation.predict(source=image)\n",
        "    black_image = np.zeros_like(image)\n",
        "    for result in results:\n",
        "        if result.masks is not None:\n",
        "            masks = result.masks.xy\n",
        "            for mask in masks:\n",
        "                mask = np.array(mask, dtype=np.int32)\n",
        "                cv2.fillPoly(black_image, [mask], (255, 255, 255))\n",
        "    binary_mask = cv2.cvtColor(black_image, cv2.COLOR_BGR2GRAY)\n",
        "    _, binary_mask = cv2.threshold(binary_mask, 1, 255, cv2.THRESH_BINARY)\n",
        "    result_image = cv2.bitwise_and(image, image, mask=binary_mask)\n",
        "    return result_image\n",
        "\n",
        "def get_image_vector(image):\n",
        "    pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)).convert(\"RGB\")\n",
        "    inputs = processor(images=pil_image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = model.get_image_features(**inputs)\n",
        "    return image_features.cpu().numpy().flatten()\n",
        "\n",
        "def find_top_k_similar_classes_with_qdrant(image, unseenmedicine_folder, k=5, top_n=1000):\n",
        "    image_vector = get_image_vector(image)\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=image_vector.tolist(),\n",
        "        limit=top_n,\n",
        "        with_payload=True,\n",
        "        with_vectors=False\n",
        "    )\n",
        "    top_results = [(result.payload.get(\"classname\", \"unknown\"), result.score) for result in search_result]\n",
        "    unique_classes = {}\n",
        "    for class_name, score in top_results:\n",
        "        if class_name not in unique_classes:\n",
        "            unique_classes[class_name] = score\n",
        "        if len(unique_classes) == k:\n",
        "            break\n",
        "    filtered_top_k_classes = sorted(unique_classes.items(), key=lambda x: x[1], reverse=True)\n",
        "    if len(filtered_top_k_classes) < k:\n",
        "        st.warning(\"Not enough unique classes found.\")\n",
        "        return\n",
        "    top_1_score = filtered_top_k_classes[0][1]\n",
        "    top_2_score = filtered_top_k_classes[1][1] if len(filtered_top_k_classes) > 1 else 0\n",
        "    if top_1_score > 0.9 and (top_1_score - top_2_score) > 0.02:\n",
        "        st.success(f\"Top prediction: {filtered_top_k_classes[0][0]} with score {top_1_score:.4f}\")\n",
        "    elif top_1_score > 0.9 and (top_1_score - top_2_score) <= 0.02:\n",
        "        st.error(\"Please take a clearer picture. >_<\")\n",
        "    elif top_1_score <= 0.9 and top_1_score >= 0.85:\n",
        "        st.error(\"Please take a clearer picture. -_-\")\n",
        "    elif top_1_score < 0.85:\n",
        "        st.error(\"I have never seen this medicine before.\")\n",
        "        if not os.path.exists(unseenmedicine_folder):\n",
        "            os.makedirs(unseenmedicine_folder)\n",
        "        # Save the image only if the condition is met\n",
        "        # shutil.copy(image_path, unseenmedicine_folder)\n",
        "        st.info(f\"Image saved to {unseenmedicine_folder}\")\n",
        "\n",
        "# Streamlit application layout\n",
        "st.title(\"Medicine Image Processing\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    # Read the image file directly from memory\n",
        "    image = Image.open(uploaded_file)\n",
        "    image = np.array(image.convert(\"RGB\"))\n",
        "\n",
        "    # Automatically start processing when an image is uploaded\n",
        "    unseenmedicine_folder = r\"C:\\Users\\Admin\\Documents\\INET\\Drug\\Medicine_StreamlitApp\\unseenmedicine\"\n",
        "    find_top_k_similar_classes_with_qdrant(image, unseenmedicine_folder, k=5)"
      ],
      "metadata": {
        "id": "vaAVQe5lNINq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
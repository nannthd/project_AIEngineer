{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/project_AIEngineer/blob/main/license_plate_EasyOCR_edit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ij01tPP-NWj",
        "outputId": "f46e33d8-d187-4696-8284-3a0619651841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3EO_2zNChu"
      },
      "source": [
        "## Install YOLOv8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aStKfvUnAV5p",
        "outputId": "e46bd09e-bf2a-45bd-fa3f-b096590633e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.196 üöÄ Python-3.10.12 torch-2.3.0+cu121 CPU (Intel Xeon 2.20GHz)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 30.2/107.7 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.0.196\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pUYyyaBGW_2A"
      },
      "outputs": [],
      "source": [
        "!pip install pytube opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G-yvS2iAd9N"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GJM44AFTvvnN"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"AWtZRbYK04sFZNNZ1t4X\")\n",
        "project = rf.workspace(\"projectaiengineer\").project(\"license-plates-gpj5f\")\n",
        "version = project.version(2)\n",
        "dataset = version.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s2npat--xtKa"
      },
      "outputs": [],
      "source": [
        "%cat {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mqm8zpidA_gG"
      },
      "outputs": [],
      "source": [
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!sed -i '$d' {dataset.location}/data.yaml\n",
        "!echo -e \"test: ../test/images\\ntrain: ../train/images\\nval: ../valid/images\" >> {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "k6p0inCjBB3E"
      },
      "outputs": [],
      "source": [
        "%cat {dataset.location}/data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szDpbeG4_MaD"
      },
      "source": [
        "# Load a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6ESksE3_TNO",
        "outputId": "a5df107b-3089-4d2a-b2c6-e627f8f18019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "fRBsG86z_MCX"
      },
      "outputs": [],
      "source": [
        "# Load a model\n",
        "model = YOLO(\"/content/drive/MyDrive/license plate/best.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQBHc5jy_gT_",
        "outputId": "3f81daa9-c3c2-4aed-e24f-81927700d7f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'number', 1: 'province'}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWqqMkXR187F"
      },
      "outputs": [],
      "source": [
        "image_test = '/content/-license-plates-2/test/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ea5anrkPOO8r"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(image_test, save=True, save_txt=True)\n",
        "pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mElbvZnke8Ie"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "directory = '/content/runs/detect/predict'\n",
        "\n",
        "# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏ô‡∏µ‡πâ‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
        "if os.path.isdir(directory):\n",
        "    # ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ\n",
        "    image_files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp'))]\n",
        "\n",
        "    # ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡∏•‡∏∞‡∏†‡∏≤‡∏û\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(directory, image_file)\n",
        "        img = mpimg.imread(image_path)\n",
        "\n",
        "        plt.imshow(img)\n",
        "        plt.title(image_file)\n",
        "        plt.axis('off')  # ‡∏õ‡∏¥‡∏î‡πÅ‡∏Å‡∏ô X ‡πÅ‡∏•‡∏∞ Y\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jMomz7vALOa"
      },
      "source": [
        "#EasyOCR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå th.txt"
      ],
      "metadata": {
        "id": "8LMjq6ouiCYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57dQ0rBWjvAU"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTQH9ctl7JMj"
      },
      "outputs": [],
      "source": [
        "!pip install easyocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXIDL43PKv8t"
      },
      "outputs": [],
      "source": [
        "!pip install opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nannthd/EasyOCR_license_plate"
      ],
      "metadata": {
        "id": "lSQcoIMeHT6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EasyOCR_license_plate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEpQdeLtHo1D",
        "outputId": "4aa09381-1b5a-47f5-bb7a-856188d042ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EasyOCR_license_plate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "bA6lgrOmHuh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "jSnmFKklQm1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install YOLOv8, EasyOCR, and OpenCV\n",
        "# !pip install ultralytics\n",
        "# !pip install easyocr\n",
        "# !pip install opencv-python-headless\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import easyocr\n",
        "\n",
        "# Step 3: Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/best.pt')  # Replace with the path to your weights file\n",
        "\n",
        "# Step 4: Load the Input Image\n",
        "input_image_path = '/content/-license-plates-2/valid/images/CH1_20150510191746_-1436_jpg.rf.5a0ed401cdbaf5df27a97f86ef0ed7df.jpg'  # Replace with the path to your input image\n",
        "image = cv2.imread(input_image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "# Step 5: Perform Inference\n",
        "results = model.predict(image_rgb, save=True)\n",
        "\n",
        "# Step 6: Initialize EasyOCR Reader for Thai Language\n",
        "reader = easyocr.Reader(['th'])  # Specify 'th' for Thai language\n",
        "\n",
        "# Step 7: Process Each Detection and Apply EasyOCR\n",
        "output_image = image_rgb.copy()\n",
        "detected_texts = {'text_number': [], 'text_province': []}  # Dictionary to store texts by category\n",
        "cropped_images = []\n",
        "\n",
        "for result in results:\n",
        "    for box in result.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Access the bounding box coordinates\n",
        "        class_id = int(box.cls)  # Access the class ID\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        # Extract the detected region and convert to grayscale\n",
        "        detected_region = cv2.cvtColor(image_rgb[y1:y2, x1:x2], cv2.COLOR_RGB2GRAY)\n",
        "        cropped_images.append((class_id, detected_region))\n",
        "\n",
        "        # Apply EasyOCR to the grayscale region\n",
        "        ocr_result = reader.readtext(detected_region)\n",
        "\n",
        "        # Collect OCR result and categorize by class\n",
        "        for (bbox, text, prob) in ocr_result:\n",
        "            if class_id == 0:\n",
        "                detected_texts['text_number'].append(text)\n",
        "            elif class_id == 1:\n",
        "                detected_texts['text_province'].append(text)\n",
        "\n",
        "# Step 8: Display the Output Image with Bounding Boxes\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Step 9: Display Cropped Images of Each Class\n",
        "for idx, (class_id, cropped_image) in enumerate(cropped_images):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f'Class ID: {class_id}')\n",
        "    plt.imshow(cropped_image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Step 10: Print the OCR Results\n",
        "print(\"Detected Texts - Number:\")\n",
        "for text in detected_texts['text_number']:\n",
        "    print(text)\n",
        "\n",
        "print(\"\\nDetected Texts - Province:\")\n",
        "for text in detected_texts['text_province']:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "9KqE3RF7j2bM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EasyOCR_license_plate\n",
        "# Step 1: Install YOLOv8, EasyOCR, and OpenCV if not installed\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import Levenshtein as lev  # ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ Levenshtein\n",
        "from Levenshtein import distance\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á EasyOCR_license_plate\n",
        "sys.path.append('/content/EasyOCR_license_plate')\n",
        "\n",
        "# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Reader ‡∏à‡∏≤‡∏Å EasyOCR_license_plate\n",
        "from easyocr import Reader\n",
        "\n",
        "# Step 3: Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/best.pt')  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå weights ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "\n",
        "# Step 4: Load the Input Image\n",
        "input_image_path = '/content/-license-plates-2/valid/images/CH1_20150510191746_-1436_jpg.rf.5a0ed401cdbaf5df27a97f86ef0ed7df.jpg'  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "image = cv2.imread(input_image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å BGR ‡πÄ‡∏õ‡πá‡∏ô RGB\n",
        "\n",
        "# Step 5: Perform Inference\n",
        "results = model.predict(image_rgb, save=True)\n",
        "\n",
        "# Step 6: Initialize EasyOCR Reader for Thai Language and specify dictionary path\n",
        "# ‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå th.txt\n",
        "th_dict_path = '/content/EasyOCR_license_plate/easyocr/dict/th.txt'\n",
        "\n",
        "# ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå\n",
        "with open(th_dict_path, 'r', encoding='utf-8') as f:\n",
        "    thai_words = [word.strip() for word in f.readlines()]\n",
        "\n",
        "# Step 7: Process Each Detection and Apply EasyOCR\n",
        "output_image = image_rgb.copy()\n",
        "detected_texts = {'text_number': [], 'text_province': []}\n",
        "cropped_images = []\n",
        "\n",
        "for result in results:\n",
        "    for box in result.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        class_id = int(box.cls)\n",
        "\n",
        "        cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        detected_region = cv2.cvtColor(image_rgb[y1:y2, x1:x2], cv2.COLOR_RGB2GRAY)\n",
        "        cropped_images.append((class_id, detected_region))\n",
        "\n",
        "        ocr_result = reader.readtext(detected_region, detail=0)\n",
        "\n",
        "        selected_texts = []\n",
        "        for text in ocr_result:\n",
        "            closest_word = text\n",
        "\n",
        "            # ‡∏´‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á Levenshtein\n",
        "            if class_id == 1:  # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Province ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        "                min_distance = float('inf')\n",
        "                best_match = text\n",
        "\n",
        "                for th_word in thai_words:\n",
        "                    dist = lev.distance(text, th_word)\n",
        "                    if dist < min_distance:\n",
        "                        min_distance = dist\n",
        "                        best_match = th_word\n",
        "\n",
        "                selected_texts.append(best_match)\n",
        "            else:\n",
        "                selected_texts.append(text)\n",
        "\n",
        "        if class_id == 0:\n",
        "            detected_texts['text_number'].extend(selected_texts)\n",
        "        elif class_id == 1:\n",
        "            detected_texts['text_province'].extend(selected_texts)\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏•‡πà‡∏≠‡∏á bounding box\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó\n",
        "for idx, (class_id, cropped_image) in enumerate(cropped_images):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f'Class ID: {class_id}')\n",
        "    plt.imshow(cropped_image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Province\n",
        "print(\"Detected Texts - Number:\")\n",
        "for text in detected_texts['text_number']:\n",
        "    print(text)\n",
        "\n",
        "print(\"\\nDetected Texts - Province:\")\n",
        "for text in detected_texts['text_province']:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "Xe6kk4Sfgiip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###‡∏õ‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û"
      ],
      "metadata": {
        "id": "n-hZve1q_SPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EasyOCR_license_plate\n",
        "# Step 1: Install YOLOv8, EasyOCR, and OpenCV if not installed\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import Levenshtein as lev  # ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏•‡∏ö‡∏£‡∏≤‡∏£‡∏µ Levenshtein\n",
        "from Levenshtein import distance\n",
        "\n",
        "# ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏™‡πâ‡∏ô‡∏ó‡∏≤‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á EasyOCR_license_plate\n",
        "sys.path.append('/content/EasyOCR_license_plate')\n",
        "\n",
        "# ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤ Reader ‡∏à‡∏≤‡∏Å EasyOCR_license_plate\n",
        "from easyocr import Reader\n",
        "\n",
        "# Step 3: Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/best.pt')  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå weights ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "\n",
        "# Step 4: Load the Input Image\n",
        "input_image_path = '/content/-license-plates-2/valid/images/CH1_20150510131335_-5922_jpg.rf.76da97dcd3c09e9fcc5c4c3bc44903a5.jpg'  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "image = cv2.imread(input_image_path)\n",
        "image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # ‡πÅ‡∏õ‡∏•‡∏á‡∏à‡∏≤‡∏Å BGR ‡πÄ‡∏õ‡πá‡∏ô RGB\n",
        "\n",
        "# Adjust contrast\n",
        "alpha = 2  # Contrast control (1.0-3.0)\n",
        "adjusted_image = cv2.convertScaleAbs(image_rgb, alpha=alpha, beta=0)\n",
        "\n",
        "# Step 5: Perform Inference\n",
        "results = model.predict(adjusted_image)\n",
        "\n",
        "# Step 6: Initialize EasyOCR Reader for Thai Language and specify dictionary path\n",
        "# ‡∏û‡∏≤‡∏ò‡πÑ‡∏ü‡∏•‡πå th.txt\n",
        "th_dict_path = '/content/EasyOCR_license_plate/easyocr/dict/th.txt'\n",
        "\n",
        "# ‡πÄ‡∏õ‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå\n",
        "with open(th_dict_path, 'r', encoding='utf-8') as f:\n",
        "    thai_words = [word.strip() for word in f.readlines()]\n",
        "\n",
        "# Step 7: Process Each Detection and Apply EasyOCR\n",
        "output_image = adjusted_image.copy()\n",
        "detected_texts = {'text_number': [], 'text_province': []}\n",
        "cropped_images = []\n",
        "\n",
        "for result in results:\n",
        "    for box in result.boxes:\n",
        "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "        class_id = int(box.cls)\n",
        "\n",
        "        cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        detected_region = cv2.cvtColor(adjusted_image[y1:y2, x1:x2], cv2.COLOR_RGB2GRAY)\n",
        "        cropped_images.append((class_id, detected_region))\n",
        "\n",
        "        ocr_result = reader.readtext(detected_region, detail=0)\n",
        "\n",
        "        selected_texts = []\n",
        "        for text in ocr_result:\n",
        "            closest_word = text\n",
        "\n",
        "            # ‡∏´‡∏≤‡∏Ñ‡∏≥‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏£‡∏∞‡∏¢‡∏∞‡∏ó‡∏≤‡∏á Levenshtein\n",
        "            if class_id == 1:  # ‡πÄ‡∏ä‡πá‡∏Ñ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Province ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
        "                min_distance = float('inf')\n",
        "                best_match = text\n",
        "\n",
        "                for th_word in thai_words:\n",
        "                    dist = lev.distance(text, th_word)\n",
        "                    if dist < min_distance:\n",
        "                        min_distance = dist\n",
        "                        best_match = th_word\n",
        "\n",
        "                selected_texts.append(best_match)\n",
        "            else:\n",
        "                selected_texts.append(text)\n",
        "\n",
        "        if class_id == 0:\n",
        "            detected_texts['text_number'].extend(selected_texts)\n",
        "        elif class_id == 1:\n",
        "            detected_texts['text_province'].extend(selected_texts)\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏•‡πà‡∏≠‡∏á bounding box\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(output_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ï‡∏±‡∏î‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó\n",
        "for idx, (class_id, cropped_image) in enumerate(cropped_images):\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f'Class ID: {class_id}')\n",
        "    plt.imshow(cropped_image, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Province\n",
        "print(\"Detected Texts - Number:\")\n",
        "for text in detected_texts['text_number']:\n",
        "    print(text)\n",
        "\n",
        "print(\"\\nDetected Texts - Province:\")\n",
        "for text in detected_texts['text_province']:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "9NAfs_9J9m-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‡∏ß‡∏µ‡∏î‡∏µ‡πÇ‡∏≠"
      ],
      "metadata": {
        "id": "p9Zp8-fXWmW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import Levenshtein as lev\n",
        "from Levenshtein import distance\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "sys.path.append('/content/EasyOCR_license_plate')\n",
        "from easyocr import Reader\n",
        "\n",
        "# Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/best.pt')\n",
        "\n",
        "# Initialize EasyOCR Reader for Thai Language\n",
        "reader = Reader(['th'], gpu=True)\n",
        "\n",
        "# Path to the video file\n",
        "input_video_path = '/content/20240611141829593_fd0afacfcf48424b988fe21221e1e674_AB8230019_0.mp4'  # ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏≤‡∏ò‡∏Ç‡∏≠‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ß‡∏µ‡∏î‡∏µ‡πÇ‡∏≠‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
        "output_video_path = '/content/output_video.mp4'\n",
        "\n",
        "# Open the video file\n",
        "video = cv2.VideoCapture(input_video_path)\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 30.0, (int(video.get(3)), int(video.get(4))))\n",
        "\n",
        "# Load the Thai words from the dictionary\n",
        "th_dict_path = '/content/EasyOCR_license_plate/easyocr/dict/th.txt'\n",
        "with open(th_dict_path, 'r', encoding='utf-8') as f:\n",
        "    thai_words = [word.strip() for word in f.readlines()]\n",
        "\n",
        "# Process the video frame by frame\n",
        "while video.isOpened():\n",
        "    ret, frame = video.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Perform Inference\n",
        "    results = model.predict(image_rgb, save=True)\n",
        "\n",
        "    output_image = image_rgb.copy()\n",
        "    detected_texts = {'text_number': [], 'text_province': []}\n",
        "    cropped_images = []\n",
        "\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            class_id = int(box.cls)\n",
        "\n",
        "            cv2.rectangle(output_image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            detected_region = cv2.cvtColor(image_rgb[y1:y2, x1:x2], cv2.COLOR_RGB2GRAY)\n",
        "            cropped_images.append((class_id, detected_region))\n",
        "\n",
        "            ocr_result = reader.readtext(detected_region, detail=0)\n",
        "\n",
        "            selected_texts = []\n",
        "            for text in ocr_result:\n",
        "                closest_word = text\n",
        "\n",
        "                if class_id == 1:\n",
        "                    min_distance = float('inf')\n",
        "                    best_match = text\n",
        "\n",
        "                    for th_word in thai_words:\n",
        "                        dist = lev.distance(text, th_word)\n",
        "                        if dist < min_distance:\n",
        "                            min_distance = dist\n",
        "                            best_match = th_word\n",
        "\n",
        "                    selected_texts.append(best_match)\n",
        "                else:\n",
        "                    selected_texts.append(text)\n",
        "\n",
        "            if class_id == 0:\n",
        "                detected_texts['text_number'].extend(selected_texts)\n",
        "            elif class_id == 1:\n",
        "                detected_texts['text_province'].extend(selected_texts)\n",
        "\n",
        "    # Convert the image back to BGR for saving\n",
        "    output_image_bgr = cv2.cvtColor(output_image, cv2.COLOR_RGB2BGR)\n",
        "    out.write(output_image_bgr)\n",
        "\n",
        "    # (Optional) Display the frame\n",
        "    cv2_imshow(output_image_bgr)\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Release everything if job is finished\n",
        "video.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÑ‡∏î‡πâ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≤‡∏Å‡πÑ‡∏ü‡∏•‡πå th.txt ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Province\n",
        "print(\"Detected Texts - Number:\")\n",
        "for text in detected_texts['text_number']:\n",
        "    print(text)\n",
        "\n",
        "print(\"\\nDetected Texts - Province:\")\n",
        "for text in detected_texts['text_province']:\n",
        "    print(text)"
      ],
      "metadata": {
        "id": "FHzTLXEwVskt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fb0tuoheVsat"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1MyA_qtPP-pS1nnMK6Rw0zeYby-jgIzJB",
      "authorship_tag": "ABX9TyNgbQwTCKrCoO7QlN4bp9s6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
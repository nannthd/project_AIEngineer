{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/project_AIEngineer/blob/main/vector_CLIP%26YOLO%26Qdrant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-GCYZPjz_vW",
        "outputId": "5e7b42f2-3f73-4a8d-9961-52936797f7ac"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "E3J7Ny31ywhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/drug/data 50 class add_augment3.zip'"
      ],
      "metadata": {
        "id": "_w83tU030ZaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Crop"
      ],
      "metadata": {
        "id": "jcIE1oiz8g5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load your YOLOv8 model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_new.pt')\n",
        "\n",
        "# Define the input path (can be a single image or a folder)\n",
        "path = '/content/data 50 class add_augment'\n",
        "\n",
        "# Define the crop directory\n",
        "crop_dir = '/content/drive/MyDrive/drug/Corp_4800'\n",
        "os.makedirs(crop_dir, exist_ok=True)\n",
        "\n",
        "# Function to perform object detection and cropping\n",
        "def detect_and_crop(image_path, save_path):\n",
        "    # Predict with the YOLO model\n",
        "    results = model(image_path)\n",
        "\n",
        "    # Parse the results\n",
        "    detected = False\n",
        "    for result in results:\n",
        "        # Extract boxes, labels, and scores\n",
        "        boxes = result.boxes.xyxy  # x1, y1, x2, y2\n",
        "        labels = result.boxes.cls  # class ids\n",
        "        scores = result.boxes.conf  # confidence scores\n",
        "\n",
        "        # Convert tensor to numpy for easier processing\n",
        "        boxes = boxes.cpu().detach().numpy()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        scores = scores.cpu().detach().numpy()\n",
        "\n",
        "        # Threshold for detection confidence\n",
        "        threshold = 0.3\n",
        "\n",
        "        # List of target labels (Replace with actual indices of your classes in the YOLO model)\n",
        "        target_labels = ['Amlopine_10mg', 'Amlopine_5mg', 'Anapril_5mg', 'Betalol-10-mg', 'Betalol-40-mg',\n",
        "                         'Blopress-16-mg', 'Blopress-8-mg', 'Blopress-Plus-16-mg', 'Blopress-Plus-8-mg',\n",
        "                         'Caduet-10_10-mg', 'Caduet-5_10-mg', 'Daonil-5-mg', 'Diamicron-MR-60-mg',\n",
        "                         'Diovan-160-mg', 'Diovan-80-mg', 'Forxiga-10-mg', 'Galvus-50-mg', 'Galvus_Met_50_1000mg',\n",
        "                         'Gliclazide_80mg', 'Gliparil-2-mg', 'Glucophage-500-mg', 'Glucophage-850-mg',\n",
        "                         'Glucophage-XR-1000-mg', 'Glucophage-XR-750-mg', 'Glyxambi-25_5-mg', 'Janumet-50_1000-mg',\n",
        "                         'Januvia-100-mg', 'Jardiance-10-mg', 'Jardiance-25-mg', 'Jardiance-Duo-12_5_1000-mg',\n",
        "                         'Lanzaar-100-mg', 'Lercadip-20-mg', 'Madiplot-10-mg', 'Madiplot_20mg',\n",
        "                         'Metoprolol_Stada_100mg', 'Micardis_40mg', 'Micardis_Plus_80_12_5mg', 'Minidiab_5mg',\n",
        "                         'Novonorm_1mg', 'Novonorm_2mg', 'Oseni_25_15mg', 'Poli_uretic', 'Prenolol_100mg',\n",
        "                         'Prenolol_25mg', 'Prenolol_50mg', 'Tanzaril_50mg', 'Utmos_15mg', 'Utmos_30mg',\n",
        "                         'Xigduo_XR_10_1000mg', 'Zanidip_10mg']\n",
        "\n",
        "        # Create a map from YOLO class index to target label\n",
        "        class_map = {i: label for i, label in enumerate(target_labels)}\n",
        "\n",
        "        # Check if any of the target labels are detected\n",
        "        for i, label in enumerate(labels):\n",
        "            if class_map[label] in target_labels and scores[i] >= threshold:\n",
        "                score = scores[i]\n",
        "                box = boxes[i]\n",
        "\n",
        "                # Get the coordinates and dimensions of the detected object\n",
        "                x1, y1, x2, y2 = box\n",
        "\n",
        "                # Open the original image\n",
        "                im = Image.open(image_path)\n",
        "\n",
        "                # Crop the image using the detected coordinates\n",
        "                im_crop = im.crop((x1, y1, x2, y2))\n",
        "\n",
        "                # Convert image mode to RGB if it's RGBA\n",
        "                if im_crop.mode == 'RGBA':\n",
        "                    im_crop = im_crop.convert('RGB')\n",
        "\n",
        "                # Save the cropped image in the original subdirectory structure\n",
        "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "                im_crop.save(save_path)\n",
        "                print(f\"Cropped image saved as '{save_path}'.\")\n",
        "                detected = True\n",
        "                return  # Exit after saving the first detected crop\n",
        "\n",
        "    if not detected:\n",
        "        print(f\"Can't detect any target objects in '{image_path}'.\")\n",
        "\n",
        "# Check if path is a file or directory\n",
        "if os.path.isfile(path):\n",
        "    save_path = os.path.join(crop_dir, os.path.relpath(path, start=path))\n",
        "    detect_and_crop(path, save_path)\n",
        "elif os.path.isdir(path):\n",
        "    # Loop through all subdirectories and files in the directory\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file_name in sorted(files):\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                save_path = os.path.join(crop_dir, os.path.relpath(file_path, start=path))\n",
        "                detect_and_crop(file_path, save_path)\n",
        "else:\n",
        "    print(f\"Invalid path: {path}\")"
      ],
      "metadata": {
        "id": "pfIDVY2I2ul7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Corp&Segment"
      ],
      "metadata": {
        "id": "KItlIXQG8kd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load your YOLOv8 segmentation model\n",
        "model = YOLO('/content/drive/MyDrive/drug/best_segment.pt')\n",
        "\n",
        "# Define the input path (can be a single image or a folder)\n",
        "path = '/content/data 50 class add_augment'\n",
        "\n",
        "# Define the crop directory\n",
        "crop_dir = '/content/drive/MyDrive/drug/Corp&Segment_4800'\n",
        "os.makedirs(crop_dir, exist_ok=True)\n",
        "\n",
        "# Function to perform object detection, cropping, and segmentation\n",
        "def detect_crop_and_segment(image_path, save_path):\n",
        "    # Predict with the YOLO model\n",
        "    results = model(image_path)\n",
        "\n",
        "    # Parse the results\n",
        "    detected = False\n",
        "    for result in results:\n",
        "        # Extract boxes, labels, and masks\n",
        "        boxes = result.boxes.xyxy  # x1, y1, x2, y2\n",
        "        labels = result.boxes.cls  # class ids\n",
        "        scores = result.boxes.conf  # confidence scores\n",
        "\n",
        "        # Convert tensor to numpy for easier processing\n",
        "        boxes = boxes.cpu().detach().numpy()\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "        scores = scores.cpu().detach().numpy()\n",
        "\n",
        "        # Check if masks exist\n",
        "        if result.masks is not None:\n",
        "            masks = result.masks.data  # segmentation masks\n",
        "            masks = masks.cpu().detach().numpy()\n",
        "        else:\n",
        "            masks = None\n",
        "\n",
        "        # Threshold for detection confidence\n",
        "        threshold = 0.3\n",
        "\n",
        "        # List of target labels (Replace with actual indices of your classes in the YOLO model)\n",
        "        target_labels = ['Amlopine_10mg', 'Amlopine_5mg', 'Anapril_5mg', 'Betalol-10-mg', 'Betalol-40-mg',\n",
        "                         'Blopress-16-mg', 'Blopress-8-mg', 'Blopress-Plus-16-mg', 'Blopress-Plus-8-mg',\n",
        "                         'Caduet-10_10-mg', 'Caduet-5_10-mg', 'Daonil-5-mg', 'Diamicron-MR-60-mg',\n",
        "                         'Diovan-160-mg', 'Diovan-80-mg', 'Forxiga-10-mg', 'Galvus-50-mg', 'Galvus_Met_50_1000mg',\n",
        "                         'Gliclazide_80mg', 'Gliparil-2-mg', 'Glucophage-500-mg', 'Glucophage-850-mg',\n",
        "                         'Glucophage-XR-1000-mg', 'Glucophage-XR-750-mg', 'Glyxambi-25_5-mg', 'Janumet-50_1000-mg',\n",
        "                         'Januvia-100-mg', 'Jardiance-10-mg', 'Jardiance-25-mg', 'Jardiance-Duo-12_5_1000-mg',\n",
        "                         'Lanzaar-100-mg', 'Lercadip-20-mg', 'Madiplot-10-mg', 'Madiplot_20mg',\n",
        "                         'Metoprolol_Stada_100mg', 'Micardis_40mg', 'Micardis_Plus_80_12_5mg', 'Minidiab_5mg',\n",
        "                         'Novonorm_1mg', 'Novonorm_2mg', 'Oseni_25_15mg', 'Poli_uretic', 'Prenolol_100mg',\n",
        "                         'Prenolol_25mg', 'Prenolol_50mg', 'Tanzaril_50mg', 'Utmos_15mg', 'Utmos_30mg',\n",
        "                         'Xigduo_XR_10_1000mg', 'Zanidip_10mg']\n",
        "\n",
        "        # Create a map from YOLO class index to target label\n",
        "        class_map = {i: label for i, label in enumerate(target_labels)}\n",
        "\n",
        "        # Check if any of the target labels are detected\n",
        "        for i, label in enumerate(labels):\n",
        "            if class_map[label] in target_labels and scores[i] >= threshold:\n",
        "                score = scores[i]\n",
        "                box = boxes[i]\n",
        "\n",
        "                # Get the coordinates and dimensions of the detected object\n",
        "                x1, y1, x2, y2 = map(int, box)\n",
        "\n",
        "                # Open the original image\n",
        "                im = Image.open(image_path)\n",
        "                im_np = np.array(im)\n",
        "\n",
        "                # Crop the image using the detected coordinates\n",
        "                im_crop = im_np[y1:y2, x1:x2]\n",
        "\n",
        "                if masks is not None:\n",
        "                    # Apply the segmentation mask to the cropped image\n",
        "                    mask = masks[i]\n",
        "                    mask_resized = cv2.resize(mask, (x2-x1, y2-y1), interpolation=cv2.INTER_NEAREST)\n",
        "                    im_segmented = cv2.bitwise_and(im_crop, im_crop, mask=mask_resized.astype(np.uint8))\n",
        "                else:\n",
        "                    im_segmented = im_crop\n",
        "\n",
        "                # Convert image back to PIL format for saving\n",
        "                im_segmented_pil = Image.fromarray(im_segmented)\n",
        "\n",
        "                # Save the segmented and cropped image in the original subdirectory structure\n",
        "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "                im_segmented_pil.save(save_path)\n",
        "                print(f\"Cropped and segmented image saved as '{save_path}'.\")\n",
        "                detected = True\n",
        "                return  # Exit after saving the first detected crop\n",
        "\n",
        "    if not detected:\n",
        "        print(f\"Can't detect any target objects in '{image_path}'.\")\n",
        "\n",
        "# Check if path is a file or directory\n",
        "if os.path.isfile(path):\n",
        "    save_path = os.path.join(crop_dir, os.path.relpath(path, start=path))\n",
        "    detect_crop_and_segment(path, save_path)\n",
        "elif os.path.isdir(path):\n",
        "    # Loop through all subdirectories and files in the directory\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file_name in sorted(files):\n",
        "            file_path = os.path.join(root, file_name)\n",
        "            if os.path.isfile(file_path):\n",
        "                save_path = os.path.join(crop_dir, os.path.relpath(file_path, start=path))\n",
        "                detect_crop_and_segment(file_path, save_path)\n",
        "else:\n",
        "    print(f\"Invalid path: {path}\")"
      ],
      "metadata": {
        "id": "t7Efd5D65YcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Qdrant"
      ],
      "metadata": {
        "id": "mnE6dNxR8rfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "2Bn2IDK0B-Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install qdrant-client"
      ],
      "metadata": {
        "id": "I8bIgg5P2Jw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import uuid\n",
        "from PIL import Image\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import PointStruct, VectorParams\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# เชื่อมต่อกับ Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "\n",
        "def create_collection(collection_name, vector_size, distance_metric):\n",
        "    try:\n",
        "        # ตรวจสอบว่าคอลเลกชันมีอยู่แล้วหรือไม่\n",
        "        collections = qdrant_client.get_collections().collections\n",
        "        existing_collections = [col.name for col in collections]\n",
        "\n",
        "        if collection_name in existing_collections:\n",
        "            print(f\"Collection {collection_name} already exists.\")\n",
        "        else:\n",
        "            # สร้างคอลเลกชัน\n",
        "            qdrant_client.create_collection(\n",
        "                collection_name=collection_name,\n",
        "                vectors_config=VectorParams(\n",
        "                    size=vector_size,  # ขนาดของเวกเตอร์ embedding\n",
        "                    distance=distance_metric  # ระยะทางที่ใช้ในการคำนวณ\n",
        "                )\n",
        "            )\n",
        "            print(f\"Collection {collection_name} created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while creating the collection: {e}\")\n",
        "\n",
        "def image_embedding(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    inputs = processor(images=img, return_tensors=\"pt\", size=(640, 640))\n",
        "    with torch.no_grad():\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy().flatten()\n",
        "    return embedding\n",
        "\n",
        "def process_images_in_folder(folder_path, collection_name):\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for idx, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        embedding = image_embedding(image_path)\n",
        "        class_name = os.path.basename(folder_path)  # ใช้ชื่อโฟลเดอร์เป็นคลาส\n",
        "        point_id = str(uuid.uuid4())  # ใช้ UUID เป็น ID\n",
        "        point = PointStruct(id=point_id, vector=embedding.tolist(), payload={\"class\": class_name, \"image_label\": image_file})\n",
        "        try:\n",
        "            qdrant_client.upsert(collection_name=collection_name, points=[point])\n",
        "            print(f\"Uploaded {image_file} to collection {collection_name} with class {class_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while uploading {image_file}: {e}\")\n",
        "\n",
        "def process_all_folders(base_folder_path):\n",
        "    collection_name = \"vector_CLIP\"  # ใช้ชื่อคอลเลกชันเดียวสำหรับเก็บข้อมูลทั้งหมด\n",
        "\n",
        "    create_collection(collection_name, vector_size=768, distance_metric='Cosine')  # ขนาดเวกเตอร์ของ CLIPModel คือ 768\n",
        "\n",
        "    subfolders = [f.path for f in os.scandir(base_folder_path) if f.is_dir()]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        process_images_in_folder(subfolder, collection_name)\n",
        "        print(f\"Embedding data from {os.path.basename(subfolder)} has been uploaded to Qdrant\")\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "base_folder_path = '/content/drive/MyDrive/drug/Corp&Segment_4800'\n",
        "process_all_folders(base_folder_path)"
      ],
      "metadata": {
        "id": "nqtNk0IayyDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HlYp1nAvGmhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import uuid\n",
        "from PIL import Image\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http.models import PointStruct, VectorParams\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # ตรวจสอบการใช้ GPU\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)  # ส่งโมเดลไปยัง GPU\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# เชื่อมต่อกับ Qdrant\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://8366dca9-9b40-481c-9a7c-102b62b118c2.europe-west3-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=\"Ft3t6L99uoTsJW8IOk5VH2byKU-BQYqiieuxXFBDp99wo75od0ddAw\",\n",
        ")\n",
        "\n",
        "def create_collection(collection_name, vector_size, distance_metric):\n",
        "    try:\n",
        "        # ตรวจสอบว่าคอลเลกชันมีอยู่แล้วหรือไม่\n",
        "        collections = qdrant_client.get_collections().collections\n",
        "        existing_collections = [col.name for col in collections]\n",
        "\n",
        "        if collection_name in existing_collections:\n",
        "            print(f\"Collection {collection_name} already exists.\")\n",
        "        else:\n",
        "            # สร้างคอลเลกชัน\n",
        "            qdrant_client.create_collection(\n",
        "                collection_name=collection_name,\n",
        "                vectors_config=VectorParams(\n",
        "                    size=vector_size,  # ขนาดของเวกเตอร์ embedding\n",
        "                    distance=distance_metric  # ระยะทางที่ใช้ในการคำนวณ\n",
        "                )\n",
        "            )\n",
        "            print(f\"Collection {collection_name} created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while creating the collection: {e}\")\n",
        "\n",
        "def image_embedding(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    inputs = processor(images=img, return_tensors=\"pt\", size=(640, 640))\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # ส่งข้อมูลไปยัง GPU\n",
        "    with torch.no_grad():\n",
        "        outputs = model.get_image_features(**inputs)\n",
        "    embedding = outputs.cpu().numpy().flatten()\n",
        "    return embedding\n",
        "\n",
        "def process_images_in_folder(folder_path, collection_name):\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    points_count = len(image_files)  # นับจำนวนภาพที่นำเข้า\n",
        "\n",
        "    for idx, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        embedding = image_embedding(image_path)\n",
        "        class_name = os.path.basename(folder_path)  # ใช้ชื่อโฟลเดอร์เป็นคลาส\n",
        "        point_id = str(uuid.uuid4())  # ใช้ UUID เป็น ID\n",
        "        point = PointStruct(id=point_id, vector=embedding.tolist(), payload={\"class\": class_name, \"image_label\": image_file})\n",
        "        try:\n",
        "            qdrant_client.upsert(collection_name=collection_name, points=[point])\n",
        "            print(f\"Uploaded {image_file} to collection {collection_name} with class {class_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while uploading {image_file}: {e}\")\n",
        "\n",
        "    return points_count  # ส่งกลับจำนวนภาพที่ประมวลผล\n",
        "\n",
        "def process_all_folders(base_folder_path):\n",
        "    collection_name = \"vector_CLIP\"  # ใช้ชื่อคอลเลกชันเดียวสำหรับเก็บข้อมูลทั้งหมด\n",
        "\n",
        "    create_collection(collection_name, vector_size=768, distance_metric='Cosine')  # ขนาดเวกเตอร์ของ CLIPModel คือ 768\n",
        "\n",
        "    subfolders = [f.path for f in os.scandir(base_folder_path) if f.is_dir()]\n",
        "\n",
        "    total_points = 0  # ตัวแปรสำหรับเก็บจำนวนภาพทั้งหมด\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        points_count = process_images_in_folder(subfolder, collection_name)\n",
        "        total_points += points_count\n",
        "        print(f\"Embedding data from {os.path.basename(subfolder)} has been uploaded to Qdrant. Points count: {points_count}\")\n",
        "\n",
        "    print(f\"Total points uploaded: {total_points}\")  # แสดงผลรวมของจำนวนภาพที่ประมวลผล\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "base_folder_path = '/content/drive/MyDrive/drug/Corp_4800'\n",
        "process_all_folders(base_folder_path)"
      ],
      "metadata": {
        "id": "-YstNnepHDV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def count_images_in_folder(folder_path):\n",
        "    total_images = 0\n",
        "\n",
        "    # ตรวจสอบว่ามีโฟลเดอร์ย่อยหรือไม่\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        # นับไฟล์ภาพในแต่ละโฟลเดอร์\n",
        "        image_files = [f for f in files if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "        total_images += len(image_files)\n",
        "\n",
        "        # แสดงชื่อโฟลเดอร์และจำนวนภาพในโฟลเดอร์นั้นๆ\n",
        "        if root != folder_path:  # ข้ามโฟลเดอร์หลัก\n",
        "            print(f\"Folder: {root}\")\n",
        "            print(f\"Number of images: {len(image_files)}\")\n",
        "\n",
        "    print(f\"\\nTotal number of images: {total_images}\")\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "base_folder_path = '/content/drive/MyDrive/drug/Corp_4800'\n",
        "count_images_in_folder(base_folder_path)"
      ],
      "metadata": {
        "id": "bRuPoSEn-8YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total number of images: 4765"
      ],
      "metadata": {
        "id": "mfsYF2KVBtuJ"
      }
    }
  ]
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzBsTZQaI5NEABU907d7zq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/project_AIEngineer/blob/main/license_plate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ij01tPP-NWj",
        "outputId": "83615a69-b63d-4041-94ca-112631d5a760"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3EO_2zNChu"
      },
      "source": [
        "## Install"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics\n",
        "!pip install easyocr\n",
        "!pip install pytube opencv-python-headless"
      ],
      "metadata": {
        "id": "kiFMTyMwgpSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test"
      ],
      "metadata": {
        "id": "8Pmh3ticgcAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwojhBh1hdVk",
        "outputId": "d02baf24-deb7-4ea7-83f8-f00641e403c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install YOLOv8, EasyOCR, and OpenCV\n",
        "# !pip install ultralytics\n",
        "# !pip install easyocr\n",
        "# !pip install opencv-python-headless\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import easyocr\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/model.pt')  # Replace with the path to your weights file\n",
        "\n",
        "# Step 4: Load the Input Video\n",
        "input_video_path = '/content/drive/MyDrive/license plate/test.mp4'  # Replace with the path to your input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Step 5: Initialize EasyOCR Reader for Thai Language\n",
        "reader = easyocr.Reader(['th'])  # Specify 'th' for Thai language\n",
        "\n",
        "# Step 6: Process Each Frame\n",
        "output_video_path = '/content/output_video.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "    # Step 7: Perform Inference\n",
        "    results = model.predict(image_rgb)\n",
        "\n",
        "    # Step 8: Process Each Detection\n",
        "    output_frame = image_rgb.copy()\n",
        "\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])  # Access the bounding box coordinates\n",
        "            class_id = int(box.cls)  # Access the class ID\n",
        "\n",
        "            # Draw bounding box\n",
        "            cv2.rectangle(output_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    # Convert the output_frame back to BGR for saving\n",
        "    output_frame_bgr = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
        "    out.write(output_frame_bgr)\n",
        "\n",
        "    # # Optionally, display the frame\n",
        "    # cv2.imshow('Output Video', output_frame_bgr)\n",
        "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    #     break\n",
        "\n",
        "# Step 9: Release the video capture and writer objects\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "PwBanOmFgXzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Apply Tracking"
      ],
      "metadata": {
        "id": "5PQXHCvlbEV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install YOLOv8, EasyOCR, and OpenCV\n",
        "# !pip install ultralytics\n",
        "# !pip install easyocr\n",
        "# !pip install opencv-python-headless\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import cv2\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "import matplotlib.pyplot as plt\n",
        "import easyocr\n",
        "import numpy as np\n",
        "\n",
        "# Step 3: Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/model.pt')  # Replace with the path to your weights file\n",
        "\n",
        "# Step 4: Load the Input Video\n",
        "input_video_path = '/content/drive/MyDrive/license plate/test.mp4'  # Replace with the path to your input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Step 5: Initialize EasyOCR Reader for Thai Language\n",
        "reader = easyocr.Reader(['th'])  # Specify 'th' for Thai language\n",
        "\n",
        "# Step 6: Initialize Video Writer\n",
        "output_video_path = '/content/output_video.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "# Step 7: Initialize Tracker Dictionary\n",
        "trackers = {}\n",
        "\n",
        "# Step 8: Process Each Frame\n",
        "frame_id = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "    # Step 9: Perform Inference on the first frame and every 10 frames\n",
        "    if frame_id % 10 == 0:\n",
        "        results = model.predict(image_rgb)\n",
        "        new_trackers = []\n",
        "\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Access the bounding box coordinates\n",
        "\n",
        "                # Initialize a new tracker for each detected object\n",
        "                tracker = cv2.TrackerCSRT_create()\n",
        "                tracker.init(frame, (x1, y1, x2 - x1, y2 - y1))\n",
        "                new_trackers.append(tracker)\n",
        "\n",
        "        # Update the trackers dictionary with new trackers\n",
        "        trackers = {i: tracker for i, tracker in enumerate(new_trackers)}\n",
        "\n",
        "    # Step 10: Update trackers and draw bounding boxes\n",
        "    output_frame = image_rgb.copy()\n",
        "    for tracker in trackers.values():\n",
        "        success, bbox = tracker.update(frame)\n",
        "        if success:\n",
        "            x, y, w, h = map(int, bbox)\n",
        "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Convert the output_frame back to BGR for saving\n",
        "    output_frame_bgr = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
        "    out.write(output_frame_bgr)\n",
        "\n",
        "    frame_id += 1\n",
        "\n",
        "    # Optionally, display the frame\n",
        "    # cv2.imshow('Output Video', output_frame_bgr)\n",
        "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    #     break\n",
        "\n",
        "# Step 11: Release the video capture and writer objects\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "3xrdLj23bEEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Crop"
      ],
      "metadata": {
        "id": "qWV04faBaKEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install YOLOv8, EasyOCR, and OpenCV\n",
        "# !pip install ultralytics\n",
        "# !pip install easyocr\n",
        "# !pip install opencv-python-headless\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import easyocr\n",
        "import os\n",
        "\n",
        "# Step 3: Load the YOLOv8 Model with Custom Weights\n",
        "model = YOLO('/content/drive/MyDrive/license plate/model.pt')  # Replace with the path to your weights file\n",
        "\n",
        "# Step 4: Load the Input Video\n",
        "input_video_path = '/content/drive/MyDrive/license plate/test2.mp4'  # Replace with the path to your input video\n",
        "cap = cv2.VideoCapture(input_video_path)\n",
        "\n",
        "# Step 5: Initialize EasyOCR Reader for Thai Language\n",
        "reader = easyocr.Reader(['th'])  # Specify 'th' for Thai language\n",
        "\n",
        "# Step 6: Initialize Video Writer\n",
        "output_video_path = '/content/output_video.mp4'\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
        "\n",
        "# Create directory to save detected objects\n",
        "save_dir = '/content/detected_objects'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Dictionary to track if a class has been saved\n",
        "saved_classes = {}\n",
        "\n",
        "# Step 7: Initialize Tracker Dictionary\n",
        "trackers = {}\n",
        "\n",
        "# Step 8: Process Each Frame\n",
        "frame_id = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "    # Step 9: Perform Inference on the first frame and every 10 frames\n",
        "    if frame_id % 20 == 0:\n",
        "        results = model.predict(image_rgb)\n",
        "        new_trackers = []\n",
        "\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Access the bounding box coordinates\n",
        "\n",
        "                # Initialize a new tracker for each detected object\n",
        "                tracker = cv2.TrackerKCF_create()\n",
        "                tracker.init(frame, (x1, y1, x2 - x1, y2 - y1))\n",
        "                new_trackers.append(tracker)\n",
        "\n",
        "                # Save the detected object as an image if it hasn't been saved already\n",
        "                class_id = int(box.cls[0])\n",
        "                class_name = result.names[class_id]\n",
        "                if class_name not in saved_classes:\n",
        "                    cropped_img = frame[y1:y2, x1:x2]\n",
        "                    save_path = os.path.join(save_dir, f'{class_name}.jpg')\n",
        "                    cv2.imwrite(save_path, cropped_img)\n",
        "                    saved_classes[class_name] = True\n",
        "\n",
        "        # Update the trackers dictionary with new trackers\n",
        "        trackers = {i: tracker for i, tracker in enumerate(new_trackers)}\n",
        "\n",
        "    # Step 10: Update trackers and draw bounding boxes\n",
        "    output_frame = image_rgb.copy()\n",
        "    for tracker in trackers.values():\n",
        "        success, bbox = tracker.update(frame)\n",
        "        if success:\n",
        "            x, y, w, h = map(int, bbox)\n",
        "            cv2.rectangle(output_frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Convert the output_frame back to BGR for saving\n",
        "    output_frame_bgr = cv2.cvtColor(output_frame, cv2.COLOR_RGB2BGR)\n",
        "    out.write(output_frame_bgr)\n",
        "\n",
        "    frame_id += 1\n",
        "\n",
        "# Step 11: Release the video capture and writer objects\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "nfT1yfxLgJN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OCR"
      ],
      "metadata": {
        "id": "oq7lx6IiiKRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nannthd/EasyOCR_license_plate"
      ],
      "metadata": {
        "id": "pBBrOvlpkMU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EasyOCR_license_plate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAgoPcWhkOJu",
        "outputId": "50b19f92-0bd7-4784-e49f-9770af2bdca6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EasyOCR_license_plate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "PbRIkpiNkP7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "tMi88AkzkRoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install EasyOCR and OpenCV if not installed\n",
        "# !pip install easyocr\n",
        "# !pip install opencv-python-headless\n",
        "\n",
        "# Step 2: Import Required Libraries\n",
        "import cv2\n",
        "import easyocr\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import Levenshtein as lev\n",
        "\n",
        "# Step 3: Initialize EasyOCR Reader for Thai Language\n",
        "reader = easyocr.Reader(['th'])  # Specify 'th' for Thai language\n",
        "\n",
        "# Step 4: Specify the Directory Containing Cropped Images\n",
        "cropped_images_dir = '/content/detected_objects'\n",
        "\n",
        "# Step 5: Specify the Dictionary Path\n",
        "th_dict_path = '/content/EasyOCR_license_plate/easyocr/dict/th.txt'\n",
        "\n",
        "# Load words from the dictionary\n",
        "with open(th_dict_path, 'r', encoding='utf-8') as f:\n",
        "    thai_words = [word.strip() for word in f.readlines()]\n",
        "\n",
        "# Step 6: Initialize a Dictionary to Store OCR Results\n",
        "detected_texts = {'text_number': [], 'text_province': []}\n",
        "\n",
        "# Step 7: Perform OCR on Each Cropped Image\n",
        "for image_name in os.listdir(cropped_images_dir):\n",
        "    if image_name.endswith('.jpg'):\n",
        "        image_path = os.path.join(cropped_images_dir, image_name)\n",
        "        class_name = os.path.splitext(image_name)[0]\n",
        "\n",
        "        # Load the image\n",
        "        cropped_image = cv2.imread(image_path)\n",
        "\n",
        "        # Apply EasyOCR to the image\n",
        "        ocr_result = reader.readtext(cropped_image, detail=0)\n",
        "\n",
        "        selected_texts = []\n",
        "        for text in ocr_result:\n",
        "            closest_word = text\n",
        "\n",
        "            # Find the closest word from the dictionary using Levenshtein distance\n",
        "            if 'province' in class_name.lower():  # Check only for Province\n",
        "                min_distance = float('inf')\n",
        "                best_match = text\n",
        "\n",
        "                for th_word in thai_words:\n",
        "                    dist = lev.distance(text, th_word)\n",
        "                    if dist < min_distance:\n",
        "                        min_distance = dist\n",
        "                        best_match = th_word\n",
        "\n",
        "                selected_texts.append(best_match)\n",
        "            else:\n",
        "                selected_texts.append(text)\n",
        "\n",
        "        ocr_text = ' '.join(selected_texts)\n",
        "        print(f\"OCR Result for {class_name}: {ocr_text}\")\n",
        "\n",
        "        # Categorize the detected text\n",
        "        if 'number' in class_name.lower():\n",
        "            detected_texts['text_number'].extend(selected_texts)\n",
        "        elif 'province' in class_name.lower():\n",
        "            detected_texts['text_province'].extend(selected_texts)\n",
        "\n",
        "# Step 8: Display the Images\n",
        "number_image_path = '/content/detected_objects/number.jpg'\n",
        "province_image_path = '/content/detected_objects/province.jpg'\n",
        "\n",
        "number_image = cv2.imread(number_image_path)\n",
        "province_image = cv2.imread(province_image_path)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(cv2.cvtColor(number_image, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Number Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(cv2.cvtColor(province_image, cv2.COLOR_BGR2RGB))\n",
        "plt.title('Province Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qt5MWTOzo_97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "K8qBs_juAI3h"
      ],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nannthd/project_AIEngineer/blob/main/vector_drug_CLIPModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/openai/clip-vit-large-patch14"
      ],
      "metadata": {
        "id": "N_KGG5_E8Ba8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "hrYdKmFMgkA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09naVNrv6xpb"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1) # we can take the softmax to get the label probabilities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Image-text similarity scores:\", logits_per_image)\n",
        "print(\"Label probabilities:\", probs)"
      ],
      "metadata": {
        "id": "98TCYaF6-6Ob",
        "outputId": "15343ba8-27c9-4720-aa09-878eb00495cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image-text similarity scores: tensor([[18.9041, 11.7159]], grad_fn=<TBackward0>)\n",
            "Label probabilities: tensor([[9.9925e-01, 7.5487e-04]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fgQbfM4R_nPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Path to your local image file\n",
        "image_path = \"/content/22_0.jpg\"\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Prepare the inputs for the model\n",
        "inputs = processor(text=[\"Novonorm1mg\", \"Novonorm2mg\", \"Novonorm\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image  # This is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1)  # Take the softmax to get the label probabilities\n",
        "\n",
        "print(\"Image-text similarity scores:\", logits_per_image)\n",
        "print(\"Label probabilities:\", probs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOUIuFn7GDYO",
        "outputId": "4cc6bc1c-535a-4187-d901-f6f398cb18c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image-text similarity scores: tensor([[26.0049, 24.9745, 19.6755]], grad_fn=<TBackward0>)\n",
            "Label probabilities: tensor([[0.7360, 0.2627, 0.0013]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Path to your local image file\n",
        "image_path = \"/content/22_0.jpg\"\n",
        "\n",
        "# Open the image\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Define multiple text labels for comparison\n",
        "text_labels = ['Amlopine_10mg', 'Amlopine_5mg', 'Anapril_5mg', 'Betalol-10-mg', 'Betalol-40-mg',\n",
        "               'Blopress-16-mg', 'Blopress-8-mg', 'Blopress-Plus-16-mg', 'Blopress-Plus-8-mg',\n",
        "               'Caduet-10_10-mg', 'Caduet-5_10-mg', 'Daonil-5-mg', 'Diamicron-MR-60-mg',\n",
        "               'Diovan-160-mg', 'Diovan-80-mg', 'Forxiga-10-mg', 'Galvus-50-mg', 'Galvus_Met_50_1000mg',\n",
        "               'Gliclazide_80mg', 'Gliparil-2-mg', 'Glucophage-500-mg', 'Glucophage-850-mg',\n",
        "               'Glucophage-XR-1000-mg', 'Glucophage-XR-750-mg', 'Glyxambi-25_5-mg', 'Janumet-50_1000-mg',\n",
        "               'Januvia-100-mg', 'Jardiance-10-mg', 'Jardiance-25-mg', 'Jardiance-Duo-12_5_1000-mg',\n",
        "               'Lanzaar-100-mg', 'Lercadip-20-mg', 'Madiplot-10-mg', 'Madiplot_20mg',\n",
        "               'Metoprolol_Stada_100mg', 'Micardis_40mg', 'Micardis_Plus_80_12_5mg', 'Minidiab_5mg',\n",
        "               'Novonorm_1mg', 'Novonorm_2mg', 'Oseni_25_15mg', 'Poli_uretic', 'Prenolol_100mg',\n",
        "               'Prenolol_25mg', 'Prenolol_50mg', 'Tanzaril_50mg', 'Utmos_15mg', 'Utmos_30mg',\n",
        "               'Xigduo_XR_10_1000mg', 'Zanidip_10mg']\n",
        "\n",
        "# Prepare the inputs for the CLIP model\n",
        "inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# Get the image-text similarity scores\n",
        "logits_per_image = outputs.logits_per_image\n",
        "\n",
        "# Compute the probabilities\n",
        "probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "# Get the top 5 similarity scores and their corresponding text labels\n",
        "top_probs, top_indices = probs.topk(5, dim=1)\n",
        "\n",
        "# Convert to lists for easier manipulation\n",
        "top_probs = top_probs.squeeze().tolist()\n",
        "top_indices = top_indices.squeeze().tolist()\n",
        "\n",
        "# Print the top 5 similarity scores and their corresponding text labels in descending order\n",
        "print(\"Top 5 similarity scores and corresponding text labels:\")\n",
        "for i, idx in enumerate(top_indices):\n",
        "    print(f\"{text_labels[idx]}: {top_probs[i]:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFgcxaUlL108",
        "outputId": "787c50a8-409e-414f-ab65-f440930f5993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 similarity scores and corresponding text labels:\n",
            "Januvia-100-mg: 0.1917\n",
            "Betalol-40-mg: 0.1078\n",
            "Gliclazide_80mg: 0.0934\n",
            "Betalol-10-mg: 0.0828\n",
            "Diovan-160-mg: 0.0784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data1"
      ],
      "metadata": {
        "id": "WjKRS-gCZvO1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## image + text 2vector"
      ],
      "metadata": {
        "id": "K8qBs_juAI3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3cxk7pea6WJT",
        "outputId": "b3cc008d-8735-49c0-9bf1-d6e3825f530d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/drug/data 50 class add_augment.zip'"
      ],
      "metadata": {
        "id": "qVmwOPgi6r8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/drug/data 50 class - 39.zip'"
      ],
      "metadata": {
        "id": "CNNffWq-n1Vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### แปลงเวกเตอร์"
      ],
      "metadata": {
        "id": "FyHbuxF7nwHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from datetime import datetime\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Path to the main directory containing 50 subfolders\n",
        "main_directory = \"/content/data 50 class - 39\"\n",
        "\n",
        "# Prepare lists to store rows for CSV\n",
        "rows = []\n",
        "\n",
        "# Start timer for the whole process\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Collect images and text labels from the subfolders and convert them to vectors\n",
        "for folder_name in os.listdir(main_directory):\n",
        "    folder_path = os.path.join(main_directory, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        text_label = folder_name\n",
        "\n",
        "        # Start timer for the current folder\n",
        "        folder_start_time = datetime.now()\n",
        "\n",
        "        # Collect all images from the current folder\n",
        "        image_files = [os.path.join(folder_path, image_name) for image_name in os.listdir(folder_path)]\n",
        "\n",
        "        for image_path in image_files:\n",
        "            # Open the image\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Prepare the inputs for the CLIP model\n",
        "            inputs = processor(text=[text_label], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            # Move inputs to the GPU\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Get the image and text embeddings\n",
        "            image_embedding = outputs.image_embeds.squeeze().cpu().tolist()\n",
        "            text_embedding = outputs.text_embeds.squeeze().cpu().tolist()\n",
        "\n",
        "            # Create a row for the CSV\n",
        "            row = [text_label, image_path] + image_embedding + text_embedding\n",
        "            rows.append(row)\n",
        "\n",
        "        # End timer for the current folder\n",
        "        folder_end_time = datetime.now()\n",
        "        folder_duration = folder_end_time - folder_start_time\n",
        "        print(f\"Converted vectors for folder: {folder_name} in {folder_duration}\")\n",
        "\n",
        "# End timer for the whole process\n",
        "end_time = datetime.now()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "# Define the header for the CSV file\n",
        "header = ['TextLabel', 'ImagePath'] + [f'ImageEmbed_{i}' for i in range(len(image_embedding))] + [f'TextEmbed_{i}' for i in range(len(text_embedding))]\n",
        "\n",
        "# Write the data to a CSV file\n",
        "csv_file_path = '/content/class_vectors39.csv'\n",
        "df = pd.DataFrame(rows, columns=header)\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"Vectors have been successfully saved to: {csv_file_path}\")\n",
        "print(f\"Total time for processing all folders: {total_duration}\")"
      ],
      "metadata": {
        "id": "m9uPgZDcLHge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cosine similarity"
      ],
      "metadata": {
        "id": "jmiELhhvw0XC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data2"
      ],
      "metadata": {
        "id": "ek3_4l-DaCB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## image + text 2vector"
      ],
      "metadata": {
        "id": "51y9FQ_maCB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e0RXWo2SaCB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101949db-c8e6-44c3-8426-dcb371cc749f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/drug/data 50 class add_augment2.zip'"
      ],
      "metadata": {
        "id": "OUZns0PFaCB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### แปลงเวกเตอร์"
      ],
      "metadata": {
        "id": "hYI8iNjxejqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from datetime import datetime\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Path to the main directory containing 50 subfolders\n",
        "main_directory = \"/content/data 50 class add_augment\"\n",
        "\n",
        "# Prepare lists to store rows for CSV\n",
        "rows = []\n",
        "\n",
        "# Start timer for the whole process\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Collect images and text labels from the subfolders and convert them to vectors\n",
        "for folder_name in os.listdir(main_directory):\n",
        "    folder_path = os.path.join(main_directory, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        text_label = folder_name\n",
        "\n",
        "        # Start timer for the current folder\n",
        "        folder_start_time = datetime.now()\n",
        "\n",
        "        # Collect all images from the current folder\n",
        "        image_files = [os.path.join(folder_path, image_name) for image_name in os.listdir(folder_path)]\n",
        "\n",
        "        for image_path in image_files:\n",
        "            # Open the image\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Prepare the inputs for the CLIP model\n",
        "            inputs = processor(text=[text_label], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            # Move inputs to the GPU\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Get the image and text embeddings\n",
        "            image_embedding = outputs.image_embeds.squeeze().cpu().tolist()\n",
        "            text_embedding = outputs.text_embeds.squeeze().cpu().tolist()\n",
        "\n",
        "            # Create a row for the CSV\n",
        "            row = [text_label, image_path] + image_embedding + text_embedding\n",
        "            rows.append(row)\n",
        "\n",
        "        # End timer for the current folder\n",
        "        folder_end_time = datetime.now()\n",
        "        folder_duration = folder_end_time - folder_start_time\n",
        "        print(f\"Converted vectors for folder: {folder_name} in {folder_duration}\")\n",
        "\n",
        "# End timer for the whole process\n",
        "end_time = datetime.now()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "# Define the header for the CSV file\n",
        "header = ['TextLabel', 'ImagePath'] + [f'ImageEmbed_{i}' for i in range(len(image_embedding))] + [f'TextEmbed_{i}' for i in range(len(text_embedding))]\n",
        "\n",
        "# Write the data to a CSV file\n",
        "csv_file_path = '/content/class_vectors_newdata.csv'\n",
        "df = pd.DataFrame(rows, columns=header)\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"Vectors have been successfully saved to: {csv_file_path}\")\n",
        "print(f\"Total time for processing all folders: {total_duration}\")"
      ],
      "metadata": {
        "id": "UVXUVy_Uejqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cosine similarity"
      ],
      "metadata": {
        "id": "_Xalj46vwqfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#data3"
      ],
      "metadata": {
        "id": "es6yVj1zu7p3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## image + text 2vector"
      ],
      "metadata": {
        "id": "g3rMiwdcu7p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "622b12e5-ae49-4601-e6c5-383e7a7c1f35",
        "id": "V0ifvg9bu7p3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip '/content/drive/MyDrive/drug/data 50 class add_augment3.zip'"
      ],
      "metadata": {
        "id": "eolwwS6Au7p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### แปลงเวกเตอร์"
      ],
      "metadata": {
        "id": "gVBdt2eLu7p4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from datetime import datetime\n",
        "\n",
        "# Check if GPU is available and use it\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Path to the main directory containing 50 subfolders\n",
        "main_directory = \"/content/data 50 class add_augment\"\n",
        "\n",
        "# Prepare lists to store rows for CSV\n",
        "rows = []\n",
        "\n",
        "# Start timer for the whole process\n",
        "start_time = datetime.now()\n",
        "\n",
        "# Collect images and text labels from the subfolders and convert them to vectors\n",
        "for folder_name in os.listdir(main_directory):\n",
        "    folder_path = os.path.join(main_directory, folder_name)\n",
        "    if os.path.isdir(folder_path):\n",
        "        text_label = folder_name\n",
        "\n",
        "        # Start timer for the current folder\n",
        "        folder_start_time = datetime.now()\n",
        "\n",
        "        # Collect all images from the current folder\n",
        "        image_files = [os.path.join(folder_path, image_name) for image_name in os.listdir(folder_path)]\n",
        "\n",
        "        for image_path in image_files:\n",
        "            # Open the image\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Prepare the inputs for the CLIP model\n",
        "            inputs = processor(text=[text_label], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "            # Move inputs to the GPU\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            # Forward pass through the model\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            # Get the image and text embeddings\n",
        "            image_embedding = outputs.image_embeds.squeeze().cpu().tolist()\n",
        "            text_embedding = outputs.text_embeds.squeeze().cpu().tolist()\n",
        "\n",
        "            # Create a row for the CSV\n",
        "            row = [text_label, image_path] + image_embedding + text_embedding\n",
        "            rows.append(row)\n",
        "\n",
        "        # End timer for the current folder\n",
        "        folder_end_time = datetime.now()\n",
        "        folder_duration = folder_end_time - folder_start_time\n",
        "        print(f\"Converted vectors for folder: {folder_name} in {folder_duration}\")\n",
        "\n",
        "# End timer for the whole process\n",
        "end_time = datetime.now()\n",
        "total_duration = end_time - start_time\n",
        "\n",
        "# Define the header for the CSV file\n",
        "header = ['TextLabel', 'ImagePath'] + [f'ImageEmbed_{i}' for i in range(len(image_embedding))] + [f'TextEmbed_{i}' for i in range(len(text_embedding))]\n",
        "\n",
        "# Write the data to a CSV file\n",
        "csv_file_path = '/content/class_vectors3.csv'\n",
        "df = pd.DataFrame(rows, columns=header)\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "print(f\"Vectors have been successfully saved to: {csv_file_path}\")\n",
        "print(f\"Total time for processing all folders: {total_duration}\")"
      ],
      "metadata": {
        "id": "D-nPA6Wvu7p5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### cosine similarity"
      ],
      "metadata": {
        "id": "HgF3HoTb1O3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### รูปต้นแบบ นักเรียนตัวอย่าง"
      ],
      "metadata": {
        "id": "zhDPwTfWx-mf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Load the CSV file containing the vectors\n",
        "csv_file_path = '/content/drive/MyDrive/drug/class_vectors3.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract the text labels and embeddings from the CSV\n",
        "text_labels = df['TextLabel'].tolist()\n",
        "image_paths = df['ImagePath'].tolist()\n",
        "image_embeddings = df[[col for col in df.columns if 'ImageEmbed_' in col]].values\n",
        "text_embeddings = df[[col for col in df.columns if 'TextEmbed_' in col]].values\n",
        "\n",
        "# Convert embeddings from lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "text_embeddings = np.array(text_embeddings)\n",
        "\n",
        "# Function to compare a new image with stored vectors\n",
        "def compare_image_with_stored_vectors(new_image_path):\n",
        "    # Open the new image\n",
        "    image = Image.open(new_image_path)\n",
        "\n",
        "    # Prepare the inputs for the CLIP model\n",
        "    inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the image embedding for the new image\n",
        "    new_image_embedding = outputs.image_embeds.squeeze().cpu().numpy().reshape(1, -1)  # Ensure 2D array\n",
        "\n",
        "    # Get the text embeddings for the new image\n",
        "    new_text_embeddings = outputs.text_embeds.squeeze().cpu().numpy()  # Ensure 2D array\n",
        "\n",
        "    # Print shapes of the new embeddings\n",
        "    print(f\"New image embedding shape: {new_image_embedding.shape}\")\n",
        "    print(f\"New text embeddings shape: {new_text_embeddings.shape}\")\n",
        "\n",
        "    # Ensure dimensions match for cosine similarity\n",
        "    if new_image_embedding.shape[1] != image_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new image embedding and stored image embeddings.\")\n",
        "    if new_text_embeddings.shape[1] != text_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new text embedding and stored text embeddings.\")\n",
        "\n",
        "    # Compute cosine similarity between new image embedding and stored image embeddings\n",
        "    image_similarities = cosine_similarity(new_image_embedding, image_embeddings)\n",
        "\n",
        "    # Compute cosine similarity between new text embedding and stored text embeddings\n",
        "    text_similarities = cosine_similarity(new_text_embeddings, text_embeddings)\n",
        "\n",
        "    # Combine results from both similarities\n",
        "    combined_similarities = np.maximum(image_similarities, text_similarities)\n",
        "\n",
        "    # Get indices based on combined similarities\n",
        "    most_similar_combined_indices = np.argsort(-combined_similarities, axis=1)[0]\n",
        "\n",
        "    # Get unique classes\n",
        "    unique_classes = set()\n",
        "    top_results = []\n",
        "\n",
        "    # Collect top results avoiding duplicate class names\n",
        "    for index in most_similar_combined_indices:\n",
        "        label = text_labels[index]\n",
        "        if label not in unique_classes:\n",
        "            unique_classes.add(label)\n",
        "            top_results.append((label, combined_similarities[0][index]))  # Fix index usage\n",
        "\n",
        "        if len(top_results) >= 6:  # Collect more than 5 results to ensure we have enough\n",
        "            break\n",
        "\n",
        "    # Print the results from 2nd to 6th most similar\n",
        "    if len(top_results) >= 6:\n",
        "        print(\"\\nRankings from 1st to 5th most similar classes:\")\n",
        "        for i in range(1, 6):  # Indices 1 to 5 are ranks 2 to 6\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    elif len(top_results) > 1:\n",
        "        print(\"\\nRankings from 1st to last available:\")\n",
        "        for i in range(1, len(top_results)):  # Indices 1 to end are ranks 2 to last available\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nNot enough results to show rankings from 1st to 5th.\")\n",
        "\n",
        "    # Display the new image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"New Image: {new_image_path}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Process all images in the base folder\n",
        "base_folder = '/content/drive/MyDrive/drug/unseenpolygon'  # Base folder containing images\n",
        "\n",
        "for image_file in os.listdir(base_folder):\n",
        "    image_path = os.path.join(base_folder, image_file)\n",
        "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for image files\n",
        "        print(f\"\\nProcessing image: {image_path}\")\n",
        "        compare_image_with_stored_vectors(image_path)"
      ],
      "metadata": {
        "id": "BvpMKGSiyCxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ผิดไป 1class"
      ],
      "metadata": {
        "id": "v0-Zg9Cz1P0Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ครอป + พื้นหลังดำ"
      ],
      "metadata": {
        "id": "BrPMSnXtuP0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Load the CSV file containing the vectors\n",
        "csv_file_path = '/content/class_vectors3.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract the text labels and embeddings from the CSV\n",
        "text_labels = df['TextLabel'].tolist()\n",
        "image_paths = df['ImagePath'].tolist()\n",
        "image_embeddings = df[[col for col in df.columns if 'ImageEmbed_' in col]].values\n",
        "text_embeddings = df[[col for col in df.columns if 'TextEmbed_' in col]].values\n",
        "\n",
        "# Convert embeddings from lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "text_embeddings = np.array(text_embeddings)\n",
        "\n",
        "# Function to compare a new image with stored vectors\n",
        "def compare_image_with_stored_vectors(new_image_path):\n",
        "    # Open the new image\n",
        "    image = Image.open(new_image_path)\n",
        "\n",
        "    # Prepare the inputs for the CLIP model\n",
        "    inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the image embedding for the new image\n",
        "    new_image_embedding = outputs.image_embeds.squeeze().cpu().numpy().reshape(1, -1)  # Ensure 2D array\n",
        "\n",
        "    # Get the text embeddings for the new image\n",
        "    new_text_embeddings = outputs.text_embeds.squeeze().cpu().numpy()  # Ensure 2D array\n",
        "\n",
        "    # Print shapes of the new embeddings\n",
        "    print(f\"New image embedding shape: {new_image_embedding.shape}\")\n",
        "    print(f\"New text embeddings shape: {new_text_embeddings.shape}\")\n",
        "\n",
        "    # Ensure dimensions match for cosine similarity\n",
        "    if new_image_embedding.shape[1] != image_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new image embedding and stored image embeddings.\")\n",
        "    if new_text_embeddings.shape[1] != text_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new text embedding and stored text embeddings.\")\n",
        "\n",
        "    # Compute cosine similarity between new image embedding and stored image embeddings\n",
        "    image_similarities = cosine_similarity(new_image_embedding, image_embeddings)\n",
        "\n",
        "    # Compute cosine similarity between new text embedding and stored text embeddings\n",
        "    text_similarities = cosine_similarity(new_text_embeddings, text_embeddings)\n",
        "\n",
        "    # Combine results from both similarities\n",
        "    combined_similarities = np.maximum(image_similarities, text_similarities)\n",
        "\n",
        "    # Get indices based on combined similarities\n",
        "    most_similar_combined_indices = np.argsort(-combined_similarities, axis=1)[0]\n",
        "\n",
        "    # Get unique classes\n",
        "    unique_classes = set()\n",
        "    top_results = []\n",
        "\n",
        "    # Collect top results avoiding duplicate class names\n",
        "    for index in most_similar_combined_indices:\n",
        "        label = text_labels[index]\n",
        "        if label not in unique_classes:\n",
        "            unique_classes.add(label)\n",
        "            top_results.append((label, combined_similarities[0][index]))  # Fix index usage\n",
        "\n",
        "        if len(top_results) >= 6:  # Collect more than 5 results to ensure we have enough\n",
        "            break\n",
        "\n",
        "    # Print the results from 2nd to 6th most similar\n",
        "    if len(top_results) >= 6:\n",
        "        print(\"\\nRankings from 1st to 5th most similar classes:\")\n",
        "        for i in range(1, 6):  # Indices 1 to 5 are ranks 2 to 6\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    elif len(top_results) > 1:\n",
        "        print(\"\\nRankings from 1st to last available:\")\n",
        "        for i in range(1, len(top_results)):  # Indices 1 to end are ranks 2 to last available\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nNot enough results to show rankings from 1st to 5th.\")\n",
        "\n",
        "    # Display the new image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"New Image: {new_image_path}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Process all images in the base folder\n",
        "base_folder = '/content/drive/MyDrive/drug/cropbest'  # Base folder containing images\n",
        "\n",
        "for image_file in os.listdir(base_folder):\n",
        "    image_path = os.path.join(base_folder, image_file)\n",
        "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for image files\n",
        "        print(f\"\\nProcessing image: {image_path}\")\n",
        "        compare_image_with_stored_vectors(image_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DHt2yOjT2xob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ไม่ครอป"
      ],
      "metadata": {
        "id": "Z2YCBsQiuIQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Load the CSV file containing the vectors\n",
        "csv_file_path = '/content/drive/MyDrive/drug/class_vectors3.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract the text labels and embeddings from the CSV\n",
        "text_labels = df['TextLabel'].tolist()\n",
        "image_paths = df['ImagePath'].tolist()\n",
        "image_embeddings = df[[col for col in df.columns if 'ImageEmbed_' in col]].values\n",
        "text_embeddings = df[[col for col in df.columns if 'TextEmbed_' in col]].values\n",
        "\n",
        "# Convert embeddings from lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "text_embeddings = np.array(text_embeddings)\n",
        "\n",
        "# Function to compare a new image with stored vectors\n",
        "def compare_image_with_stored_vectors(new_image_path):\n",
        "    # Open the new image\n",
        "    image = Image.open(new_image_path)\n",
        "\n",
        "    # Prepare the inputs for the CLIP model\n",
        "    inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the image embedding for the new image\n",
        "    new_image_embedding = outputs.image_embeds.squeeze().cpu().numpy().reshape(1, -1)  # Ensure 2D array\n",
        "\n",
        "    # Get the text embeddings for the new image\n",
        "    new_text_embeddings = outputs.text_embeds.squeeze().cpu().numpy()  # Ensure 2D array\n",
        "\n",
        "    # Print shapes of the new embeddings\n",
        "    print(f\"New image embedding shape: {new_image_embedding.shape}\")\n",
        "    print(f\"New text embeddings shape: {new_text_embeddings.shape}\")\n",
        "\n",
        "    # Ensure dimensions match for cosine similarity\n",
        "    if new_image_embedding.shape[1] != image_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new image embedding and stored image embeddings.\")\n",
        "    if new_text_embeddings.shape[1] != text_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new text embedding and stored text embeddings.\")\n",
        "\n",
        "    # Compute cosine similarity between new image embedding and stored image embeddings\n",
        "    image_similarities = cosine_similarity(new_image_embedding, image_embeddings)\n",
        "\n",
        "    # Compute cosine similarity between new text embedding and stored text embeddings\n",
        "    text_similarities = cosine_similarity(new_text_embeddings, text_embeddings)\n",
        "\n",
        "    # Combine results from both similarities\n",
        "    combined_similarities = np.maximum(image_similarities, text_similarities)\n",
        "\n",
        "    # Get indices based on combined similarities\n",
        "    most_similar_combined_indices = np.argsort(-combined_similarities, axis=1)[0]\n",
        "\n",
        "    # Get unique classes\n",
        "    unique_classes = set()\n",
        "    top_results = []\n",
        "\n",
        "    # Collect top results avoiding duplicate class names\n",
        "    for index in most_similar_combined_indices:\n",
        "        label = text_labels[index]\n",
        "        if label not in unique_classes:\n",
        "            unique_classes.add(label)\n",
        "            top_results.append((label, combined_similarities[0][index]))  # Fix index usage\n",
        "\n",
        "        if len(top_results) >= 6:  # Collect more than 5 results to ensure we have enough\n",
        "            break\n",
        "\n",
        "    # Print the results from 2nd to 6th most similar\n",
        "    if len(top_results) >= 6:\n",
        "        print(\"\\nRankings from 1st to 5th most similar classes:\")\n",
        "        for i in range(1, 6):  # Indices 1 to 5 are ranks 2 to 6\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    elif len(top_results) > 1:\n",
        "        print(\"\\nRankings from 1st to last available:\")\n",
        "        for i in range(1, len(top_results)):  # Indices 1 to end are ranks 2 to last available\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nNot enough results to show rankings from 1st to 5th.\")\n",
        "\n",
        "    # Display the new image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"New Image: {new_image_path}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Process all images in the base folder\n",
        "base_folder = '/content/drive/MyDrive/drug/test'  # Base folder containing images\n",
        "\n",
        "for image_file in os.listdir(base_folder):\n",
        "    image_path = os.path.join(base_folder, image_file)\n",
        "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for image files\n",
        "        print(f\"\\nProcessing image: {image_path}\")\n",
        "        compare_image_with_stored_vectors(image_path)"
      ],
      "metadata": {
        "id": "jjzG1ajj5btr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ผิดไป 13class"
      ],
      "metadata": {
        "id": "BFp_uDwnxMU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ครอป"
      ],
      "metadata": {
        "id": "ZP2zH9M7uauG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "id": "I0vGioOrn1QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Paths\n",
        "model_path = \"/content/drive/MyDrive/drug/model.pt\"\n",
        "input_folder = \"/content/drive/MyDrive/drug/test\"  # Update with your folder containing images\n",
        "output_folder = \"/content/drive/MyDrive/drug/crop\" # Update with your desired output folder\n",
        "\n",
        "# Initialize YOLO model\n",
        "model = YOLO(model_path)\n",
        "\n",
        "# Ensure output folder exists, create if it doesn't\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Process each image in the input folder\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  # Adjust for image types you have\n",
        "        image_path = os.path.join(input_folder, filename)\n",
        "        output_image_path = os.path.join(output_folder, f\"cropped_{filename}\")  # Adjust naming as needed\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Perform detection\n",
        "        results = model(image)\n",
        "\n",
        "        # Process each result (usually one for YOLOv8 per image)\n",
        "        for result in results:\n",
        "            boxes = result.boxes\n",
        "            labels = result.names\n",
        "\n",
        "            # Crop and save each detected object\n",
        "            for i, box in enumerate(boxes):\n",
        "                x1, y1, x2, y2 = box.xyxy[0]  # Assuming the first detection, adjust as needed\n",
        "                confidence = box.conf[0]\n",
        "                cls = int(box.cls[0])\n",
        "\n",
        "                # Crop the image using the bounding box coordinates\n",
        "                cropped_image = image[int(y1):int(y2), int(x1):int(x2)]\n",
        "\n",
        "                # Save the cropped image with a new file name in the output folder\n",
        "                output_image_name = f\"cropped_{filename.split('.')[0]}_{i}.jpg\"  # Unique filename\n",
        "                output_image_path = os.path.join(output_folder, output_image_name)\n",
        "                cv2.imwrite(output_image_path, cropped_image)\n",
        "\n",
        "                print(f\"Cropped image saved: {output_image_path}\")\n",
        "\n",
        "print(\"All images processed and cropped.\")"
      ],
      "metadata": {
        "id": "vQB6sLf_nzSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the CLIP model and processor\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "# Load the CSV file containing the vectors\n",
        "csv_file_path = '/content/drive/MyDrive/drug/class_vectors3.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Extract the text labels and embeddings from the CSV\n",
        "text_labels = df['TextLabel'].tolist()\n",
        "image_paths = df['ImagePath'].tolist()\n",
        "image_embeddings = df[[col for col in df.columns if 'ImageEmbed_' in col]].values\n",
        "text_embeddings = df[[col for col in df.columns if 'TextEmbed_' in col]].values\n",
        "\n",
        "# Convert embeddings from lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings)\n",
        "text_embeddings = np.array(text_embeddings)\n",
        "\n",
        "# Function to compare a new image with stored vectors\n",
        "def compare_image_with_stored_vectors(new_image_path):\n",
        "    # Open the new image\n",
        "    image = Image.open(new_image_path)\n",
        "\n",
        "    # Prepare the inputs for the CLIP model\n",
        "    inputs = processor(text=text_labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Forward pass through the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get the image embedding for the new image\n",
        "    new_image_embedding = outputs.image_embeds.squeeze().cpu().numpy().reshape(1, -1)  # Ensure 2D array\n",
        "\n",
        "    # Get the text embeddings for the new image\n",
        "    new_text_embeddings = outputs.text_embeds.squeeze().cpu().numpy()  # Ensure 2D array\n",
        "\n",
        "    # Print shapes of the new embeddings\n",
        "    print(f\"New image embedding shape: {new_image_embedding.shape}\")\n",
        "    print(f\"New text embeddings shape: {new_text_embeddings.shape}\")\n",
        "\n",
        "    # Ensure dimensions match for cosine similarity\n",
        "    if new_image_embedding.shape[1] != image_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new image embedding and stored image embeddings.\")\n",
        "    if new_text_embeddings.shape[1] != text_embeddings.shape[1]:\n",
        "        raise ValueError(\"Dimension mismatch between new text embedding and stored text embeddings.\")\n",
        "\n",
        "    # Compute cosine similarity between new image embedding and stored image embeddings\n",
        "    image_similarities = cosine_similarity(new_image_embedding, image_embeddings)\n",
        "\n",
        "    # Compute cosine similarity between new text embedding and stored text embeddings\n",
        "    text_similarities = cosine_similarity(new_text_embeddings, text_embeddings)\n",
        "\n",
        "    # Combine results from both similarities\n",
        "    combined_similarities = np.maximum(image_similarities, text_similarities)\n",
        "\n",
        "    # Get indices based on combined similarities\n",
        "    most_similar_combined_indices = np.argsort(-combined_similarities, axis=1)[0]\n",
        "\n",
        "    # Get unique classes\n",
        "    unique_classes = set()\n",
        "    top_results = []\n",
        "\n",
        "    # Collect top results avoiding duplicate class names\n",
        "    for index in most_similar_combined_indices:\n",
        "        label = text_labels[index]\n",
        "        if label not in unique_classes:\n",
        "            unique_classes.add(label)\n",
        "            top_results.append((label, combined_similarities[0][index]))  # Fix index usage\n",
        "\n",
        "        if len(top_results) >= 6:  # Collect more than 5 results to ensure we have enough\n",
        "            break\n",
        "\n",
        "    # Print the results from 2nd to 6th most similar\n",
        "    if len(top_results) >= 6:\n",
        "        print(\"\\nRankings from 1st to 5th most similar classes:\")\n",
        "        for i in range(1, 6):  # Indices 1 to 5 are ranks 2 to 6\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    elif len(top_results) > 1:\n",
        "        print(\"\\nRankings from 1st to last available:\")\n",
        "        for i in range(1, len(top_results)):  # Indices 1 to end are ranks 2 to last available\n",
        "            label, similarity = top_results[i]\n",
        "            print(f\"{label}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(\"\\nNot enough results to show rankings from 1st to 5th.\")\n",
        "\n",
        "    # Display the new image\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(image)\n",
        "    plt.title(f\"New Image: {new_image_path}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Process all images in the base folder\n",
        "base_folder = '/content/drive/MyDrive/drug/crop'  # Base folder containing images\n",
        "\n",
        "for image_file in os.listdir(base_folder):\n",
        "    image_path = os.path.join(base_folder, image_file)\n",
        "    if image_file.lower().endswith(('.png', '.jpg', '.jpeg')):  # Check for image files\n",
        "        print(f\"\\nProcessing image: {image_path}\")\n",
        "        compare_image_with_stored_vectors(image_path)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S-DFH-yB1Psg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ผิดไป 10class"
      ],
      "metadata": {
        "id": "3l1-IZHHsw8x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# image2vector"
      ],
      "metadata": {
        "id": "m8VcoOiPXay6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##224*224"
      ],
      "metadata": {
        "id": "P85VZeB6x7dN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###50cls"
      ],
      "metadata": {
        "id": "wkSAvMXPx953"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def image_embedding(path):\n",
        "    img = Image.open(path).convert('RGB').resize((224, 224))  # ปรับขนาดภาพให้เข้ากับ CLIP\n",
        "    inputs = processor(images=img, return_tensors=\"pt\")\n",
        "    outputs = model.get_image_features(**inputs)\n",
        "    avg_embedding = outputs.squeeze().detach().numpy()  # ค่า embedding ของภาพ\n",
        "\n",
        "    curr_df = pd.DataFrame(avg_embedding).T\n",
        "    return curr_df\n",
        "\n",
        "def process_images_in_folder(folder_path, output_csv_path):\n",
        "    pdEmbedded = pd.DataFrame()\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for idx, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        embedded = image_embedding(image_path)\n",
        "        embedded['ID'] = image_file  # ใช้ชื่อไฟล์เป็น ID\n",
        "        pdEmbedded = pd.concat([pdEmbedded, embedded], ignore_index=True)\n",
        "\n",
        "    # สร้างโฟลเดอร์สำหรับ output_csv_path\n",
        "    output_folder = os.path.dirname(output_csv_path)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # บันทึก DataFrame ลงในไฟล์ CSV\n",
        "    pdEmbedded.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# ฟังก์ชันสำหรับการวนลูปโฟลเดอร์\n",
        "def process_all_folders(base_folder_path, output_base_folder):\n",
        "    subfolders = [f.path for f in os.scandir(base_folder_path) if f.is_dir()]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        subfolder_name = os.path.basename(subfolder)\n",
        "        output_csv_path = os.path.join(output_base_folder, f\"{subfolder_name}.csv\")\n",
        "        process_images_in_folder(subfolder, output_csv_path)\n",
        "        print(f\"Embedding data for {subfolder_name} has been saved to {output_csv_path}\")\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "base_folder_path = \"/content/data 50 class add_augment - Copy\"\n",
        "output_base_folder = \"/content/drug50cls_CLIPModel\"\n",
        "os.makedirs(output_base_folder, exist_ok=True)  # สร้างโฟลเดอร์หลักถ้ายังไม่มี\n",
        "\n",
        "process_all_folders(base_folder_path, output_base_folder)"
      ],
      "metadata": {
        "id": "GnydBJwnrmgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# โหลดข้อมูลเวกเตอร์224*224\n",
        "# Export Data drug50cls\n",
        "# Define the folder you want to zip and download\n",
        "import shutil\n",
        "\n",
        "folder_to_download = '/content/drug50cls_CLIPModel'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive('/content/drive/MyDrive/drug/drug50cls_CLIPModel', 'zip', folder_to_download)\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/drug/drug50cls_CLIPModel.zip')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "4tRUs3EKxsFp",
        "outputId": "25099c59-8089-4f30-e40a-a7040aeba146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a6121b55-a46f-41fd-927f-d551dbb9f9f7\", \"drug50cls_CLIPModel.zip\", 5032142)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cosine similarity"
      ],
      "metadata": {
        "id": "mbY_6WM_yFVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "\n",
        "# ย้ายโมเดลไปยัง GPU ถ้ามี\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def image_embedding(path):\n",
        "    try:\n",
        "        img = Image.open(path).convert('RGB').resize((224, 224))  # ปรับขนาดภาพให้เข้ากับ CLIP\n",
        "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "        avg_embedding = outputs.squeeze().cpu().detach().numpy()  # ค่า embedding ของภาพ\n",
        "        curr_df = pd.DataFrame(avg_embedding).T\n",
        "        return curr_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def load_embeddings_from_csv(csv_folder):\n",
        "    all_embeddings = {}\n",
        "    for csv_file in os.listdir(csv_folder):\n",
        "        if csv_file.endswith('.csv'):\n",
        "            class_name = os.path.splitext(csv_file)[0]  # ใช้ชื่อไฟล์ CSV เป็นชื่อคลาส\n",
        "            csv_path = os.path.join(csv_folder, csv_file)\n",
        "            df = pd.read_csv(csv_path)\n",
        "            # ดรอปคอลัมน์ ID และเก็บเฉพาะเวกเตอร์ของคลาส\n",
        "            embeddings_no_id = df.drop(['ID'], axis=1)\n",
        "            all_embeddings[class_name] = embeddings_no_id\n",
        "    return all_embeddings\n",
        "\n",
        "def find_most_similar_classes(new_image_path, all_embeddings):\n",
        "    new_embedding = image_embedding(new_image_path)\n",
        "    similarity_scores = {}\n",
        "\n",
        "    for class_name, embeddings_df in all_embeddings.items():\n",
        "        # คำนวณค่า similarity ระหว่าง embedding ของรูปภาพใหม่กับ embeddings ของคลาส\n",
        "        similarity_score = cosine_similarity(new_embedding, embeddings_df)\n",
        "        max_similarity_score = similarity_score.max()  # หา similarity ที่สูงที่สุดในคลาส\n",
        "        similarity_scores[class_name] = max_similarity_score\n",
        "\n",
        "    sorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_5_similar_classes = sorted_similarity[:5]\n",
        "\n",
        "    return top_5_similar_classes\n",
        "\n",
        "# โหลด embedding จากโฟลเดอร์ที่เก็บไฟล์ CSV\n",
        "csv_folder_path = '/content/drug50cls_CLIPModel'\n",
        "all_embeddings = load_embeddings_from_csv(csv_folder_path)\n",
        "\n",
        "# รูปภาพที่ต้องการทดสอบ\n",
        "new_image_path = '/content/11_0.jpg'\n",
        "\n",
        "# ค้นหาคลาสที่มีความคล้ายกันที่สุด 5 อันดับ\n",
        "similar_classes = find_most_similar_classes(new_image_path, all_embeddings)\n",
        "\n",
        "# แสดงผลลัพธ์\n",
        "print(\"Top 5 most similar classes:\")\n",
        "for rank, (class_name, similarity_score) in enumerate(similar_classes, start=1):\n",
        "    print(f\"{rank}. Class: {class_name}, Similarity Score: {similarity_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJH58tSMyGac",
        "outputId": "5158fa40-06d4-49b4-f207-6a438346abdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most similar classes:\n",
            "1. Class: Novonorm1mg, Similarity Score: 0.9259026699786254\n",
            "2. Class: Novonorm2mg, Similarity Score: 0.9195596741540191\n",
            "3. Class: Blopress8mg, Similarity Score: 0.9005363853770779\n",
            "4. Class: Daonil5mg, Similarity Score: 0.9001865754719013\n",
            "5. Class: Januvia100mg, Similarity Score: 0.8908871779090606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "\n",
        "# ย้ายโมเดลไปยัง GPU ถ้ามี\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def image_embedding(path):\n",
        "    try:\n",
        "        img = Image.open(path).convert('RGB').resize((224, 224))  # ปรับขนาดภาพให้เข้ากับ CLIP\n",
        "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "        avg_embedding = outputs.squeeze().cpu().detach().numpy()  # ค่า embedding ของภาพ\n",
        "        curr_df = pd.DataFrame(avg_embedding).T\n",
        "        return curr_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def load_embeddings_from_csv(csv_folder):\n",
        "    all_embeddings = {}\n",
        "    for csv_file in os.listdir(csv_folder):\n",
        "        if csv_file.endswith('.csv'):\n",
        "            class_name = os.path.splitext(csv_file)[0]  # ใช้ชื่อไฟล์ CSV เป็นชื่อคลาส\n",
        "            csv_path = os.path.join(csv_folder, csv_file)\n",
        "            df = pd.read_csv(csv_path)\n",
        "            # ดรอปคอลัมน์ ID และเก็บเฉพาะเวกเตอร์ของคลาส\n",
        "            embeddings_no_id = df.drop(['ID'], axis=1)\n",
        "            all_embeddings[class_name] = embeddings_no_id\n",
        "    return all_embeddings\n",
        "\n",
        "def find_most_similar_classes(new_image_path, all_embeddings):\n",
        "    new_embedding = image_embedding(new_image_path)\n",
        "    similarity_scores = {}\n",
        "\n",
        "    for class_name, embeddings_df in all_embeddings.items():\n",
        "        # คำนวณค่า similarity ระหว่าง embedding ของรูปภาพใหม่กับ embeddings ของคลาส\n",
        "        similarity_score = cosine_similarity(new_embedding, embeddings_df)\n",
        "        max_similarity_score = similarity_score.max()  # หา similarity ที่สูงที่สุดในคลาส\n",
        "        similarity_scores[class_name] = max_similarity_score\n",
        "\n",
        "    sorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_5_similar_classes = sorted_similarity[:5]\n",
        "\n",
        "    return top_5_similar_classes\n",
        "\n",
        "def process_images_in_folder(folder_path, all_embeddings):\n",
        "    results = []\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for image_file in image_files:\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        similar_classes = find_most_similar_classes(image_path, all_embeddings)\n",
        "        results.append({'Image': image_path, 'Top 5 Similar Classes': similar_classes})\n",
        "\n",
        "    return results\n",
        "\n",
        "def display_results(results):\n",
        "    for result in results:\n",
        "        # แสดงภาพ\n",
        "        img = Image.open(result['Image'])\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 1, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"Image: {os.path.basename(result['Image'])}\")\n",
        "\n",
        "        # แสดงผลลัพธ์\n",
        "        print(f\"Image: {os.path.basename(result['Image'])}\")\n",
        "        print(\"Top 5 most similar classes:\")\n",
        "        for rank, (class_name, similarity_score) in enumerate(result['Top 5 Similar Classes'], start=1):\n",
        "            print(f\"{rank}. Class: {class_name}, Similarity Score: {similarity_score}\")\n",
        "        print()\n",
        "        plt.show()\n",
        "\n",
        "# โหลด embedding จากโฟลเดอร์ที่เก็บไฟล์ CSV\n",
        "csv_folder_path = '/content/drug50cls_CLIPModel'\n",
        "all_embeddings = load_embeddings_from_csv(csv_folder_path)\n",
        "\n",
        "# โฟลเดอร์ที่มีภาพที่ต้องการทดสอบ\n",
        "test_images_folder = '/content/drive/MyDrive/drug/Corp_test'\n",
        "\n",
        "# ประมวลผลภาพทั้งหมดในโฟลเดอร์\n",
        "results = process_images_in_folder(test_images_folder, all_embeddings)\n",
        "\n",
        "# แสดงผลลัพธ์\n",
        "display_results(results)"
      ],
      "metadata": {
        "id": "FGuKLRtzy-tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##640*640"
      ],
      "metadata": {
        "id": "X8dBWMNw0Cyb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###50cls"
      ],
      "metadata": {
        "id": "BsZaj6fk0LQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def image_embedding(path):\n",
        "    img = Image.open(path).convert('RGB').resize((640, 640))  # ปรับขนาดภาพให้เข้ากับ CLIP\n",
        "    inputs = processor(images=img, return_tensors=\"pt\")\n",
        "    outputs = model.get_image_features(**inputs)\n",
        "    avg_embedding = outputs.squeeze().detach().numpy()  # ค่า embedding ของภาพ\n",
        "\n",
        "    curr_df = pd.DataFrame(avg_embedding).T\n",
        "    return curr_df\n",
        "\n",
        "def process_images_in_folder(folder_path, output_csv_path):\n",
        "    pdEmbedded = pd.DataFrame()\n",
        "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "    for idx, image_file in enumerate(image_files):\n",
        "        image_path = os.path.join(folder_path, image_file)\n",
        "        embedded = image_embedding(image_path)\n",
        "        embedded['ID'] = image_file  # ใช้ชื่อไฟล์เป็น ID\n",
        "        pdEmbedded = pd.concat([pdEmbedded, embedded], ignore_index=True)\n",
        "\n",
        "    # สร้างโฟลเดอร์สำหรับ output_csv_path\n",
        "    output_folder = os.path.dirname(output_csv_path)\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # บันทึก DataFrame ลงในไฟล์ CSV\n",
        "    pdEmbedded.to_csv(output_csv_path, index=False)\n",
        "\n",
        "# ฟังก์ชันสำหรับการวนลูปโฟลเดอร์\n",
        "def process_all_folders(base_folder_path, output_base_folder):\n",
        "    subfolders = [f.path for f in os.scandir(base_folder_path) if f.is_dir()]\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        subfolder_name = os.path.basename(subfolder)\n",
        "        output_csv_path = os.path.join(output_base_folder, f\"{subfolder_name}.csv\")\n",
        "        process_images_in_folder(subfolder, output_csv_path)\n",
        "        print(f\"Embedding data for {subfolder_name} has been saved to {output_csv_path}\")\n",
        "\n",
        "# ตัวอย่างการใช้งาน\n",
        "base_folder_path = \"/content/data 50 class add_augment - Copy\"\n",
        "output_base_folder = \"/content/drug50cls_CLIPModel_640\"\n",
        "os.makedirs(output_base_folder, exist_ok=True)  # สร้างโฟลเดอร์หลักถ้ายังไม่มี\n",
        "\n",
        "process_all_folders(base_folder_path, output_base_folder)"
      ],
      "metadata": {
        "id": "pY_kzJKS0LQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# โหลดข้อมูลเวกเตอร์640*640\n",
        "# Export Data drug50cls\n",
        "# Define the folder you want to zip and download\n",
        "import shutil\n",
        "\n",
        "folder_to_download = '/content/drug50cls_CLIPModel_640'\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive('/content/drive/MyDrive/drug/drug50cls_CLIPModel_640', 'zip', folder_to_download)\n",
        "\n",
        "# Download the zip file\n",
        "from google.colab import files\n",
        "files.download('/content/drive/MyDrive/drug/drug50cls_CLIPModel_640.zip')"
      ],
      "metadata": {
        "id": "z0H6TMa80LQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Cosine similarity"
      ],
      "metadata": {
        "id": "3QZruxRt3hKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "# ใช้โมเดล CLIP ของ Hugging Face ในการทำ embedding\n",
        "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
        "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
        "\n",
        "# ย้ายโมเดลไปยัง GPU ถ้ามี\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def image_embedding(path):\n",
        "    try:\n",
        "        img = Image.open(path).convert('RGB').resize((640, 640))  # ปรับขนาดภาพให้เข้ากับ CLIP\n",
        "        inputs = processor(images=img, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model.get_image_features(**inputs)\n",
        "        avg_embedding = outputs.squeeze().cpu().detach().numpy()  # ค่า embedding ของภาพ\n",
        "        curr_df = pd.DataFrame(avg_embedding).T\n",
        "        return curr_df\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image {path}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def load_embeddings_from_csv(csv_folder):\n",
        "    all_embeddings = {}\n",
        "    for csv_file in os.listdir(csv_folder):\n",
        "        if csv_file.endswith('.csv'):\n",
        "            class_name = os.path.splitext(csv_file)[0]  # ใช้ชื่อไฟล์ CSV เป็นชื่อคลาส\n",
        "            csv_path = os.path.join(csv_folder, csv_file)\n",
        "            df = pd.read_csv(csv_path)\n",
        "            # ดรอปคอลัมน์ ID และเก็บเฉพาะเวกเตอร์ของคลาส\n",
        "            embeddings_no_id = df.drop(['ID'], axis=1)\n",
        "            all_embeddings[class_name] = embeddings_no_id\n",
        "    return all_embeddings\n",
        "\n",
        "def find_most_similar_classes(new_image_path, all_embeddings):\n",
        "    new_embedding = image_embedding(new_image_path)\n",
        "    similarity_scores = {}\n",
        "\n",
        "    for class_name, embeddings_df in all_embeddings.items():\n",
        "        # คำนวณค่า similarity ระหว่าง embedding ของรูปภาพใหม่กับ embeddings ของคลาส\n",
        "        similarity_score = cosine_similarity(new_embedding, embeddings_df)\n",
        "        max_similarity_score = similarity_score.max()  # หา similarity ที่สูงที่สุดในคลาส\n",
        "        similarity_scores[class_name] = max_similarity_score\n",
        "\n",
        "    sorted_similarity = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_5_similar_classes = sorted_similarity[:5]\n",
        "\n",
        "    return top_5_similar_classes\n",
        "\n",
        "# โหลด embedding จากโฟลเดอร์ที่เก็บไฟล์ CSV\n",
        "csv_folder_path = '/content/drug50cls_CLIPModel_640'\n",
        "all_embeddings = load_embeddings_from_csv(csv_folder_path)\n",
        "\n",
        "# รูปภาพที่ต้องการทดสอบ\n",
        "new_image_path = '/content/11_0.jpg'\n",
        "\n",
        "# ค้นหาคลาสที่มีความคล้ายกันที่สุด 5 อันดับ\n",
        "similar_classes = find_most_similar_classes(new_image_path, all_embeddings)\n",
        "\n",
        "# แสดงผลลัพธ์\n",
        "print(\"Top 5 most similar classes:\")\n",
        "for rank, (class_name, similarity_score) in enumerate(similar_classes, start=1):\n",
        "    print(f\"{rank}. Class: {class_name}, Similarity Score: {similarity_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b0b916-6db3-4f68-b5f9-730e9414298d",
        "id": "aBMo71gF3hKC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most similar classes:\n",
            "1. Class: Novonorm1mg, Similarity Score: 0.9256031560681242\n",
            "2. Class: Novonorm2mg, Similarity Score: 0.9175729119575181\n",
            "3. Class: Blopress8mg, Similarity Score: 0.9035452809524182\n",
            "4. Class: Daonil5mg, Similarity Score: 0.8999247377982782\n",
            "5. Class: Januvia100mg, Similarity Score: 0.8927445084949714\n"
          ]
        }
      ]
    }
  ]
}